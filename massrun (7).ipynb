{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_963737/340142890.py:384: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(TRAIN_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Num malignant: 81\n",
      "Validation - Num benign: 80133\n",
      "Training - Num malignant: 31512\n",
      "Training - Num benign: 320533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_963737/340142890.py:482: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.fulldata = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Training set size: 63024\n",
      "Validation set size: 80214\n",
      "EPOCH: 1\n",
      "Epoch [1/10] Train Loss: 1.9855 Val Loss: 0.6968\n",
      "Val TN: 38152 FN: 19 \n",
      "TP: 62 FP: 41981\n",
      "Val Accuracy: 0.4764\n",
      "AUC: 0.7231\n",
      "pAUC: 0.0544\n",
      "f1 score: 0.0029\n",
      "\n",
      "Training set size: 63024\n",
      "Validation set size: 80214\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 668\u001b[0m\n\u001b[1;32m    665\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    666\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 668\u001b[0m     avgloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    669\u001b[0m avgloss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    671\u001b[0m resnet\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import torchvision\n",
    "import timm\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HighTPRPartialAUCLoss(nn.Module):\n",
    "    def __init__(self, tpr_threshold=0.8, lambda_rank=1.0, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.tpr_threshold = tpr_threshold\n",
    "        self.lambda_rank = lambda_rank\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.view(-1).to(self.device)\n",
    "        targets = targets.view(-1).to(self.device)\n",
    "    \n",
    "        # Numerical stability\n",
    "        logits = torch.clamp(logits, min=-50, max=50)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "    \n",
    "        pos_scores = logits[targets == 1]\n",
    "        neg_scores = logits[targets == 0]\n",
    "    \n",
    "        if len(pos_scores) == 0 or len(neg_scores) == 0:\n",
    "            return bce_loss\n",
    "    \n",
    "        tpr_threshold_idx = min(int((1-self.tpr_threshold) * len(pos_scores)), len(pos_scores) - 1)\n",
    "        pos_scores_sorted, _ = torch.sort(pos_scores)\n",
    "        tpr_threshold_score = pos_scores_sorted[tpr_threshold_idx]\n",
    "    \n",
    "        low_tpr_pos = pos_scores[pos_scores < tpr_threshold_score]\n",
    "        k = max(1, int(0.2 * len(neg_scores)))\n",
    "        high_conf_neg, _ = torch.topk(neg_scores, k=k, largest=True)\n",
    "    \n",
    "        if len(low_tpr_pos) == 0 or len(high_conf_neg) == 0:\n",
    "            return bce_loss\n",
    "    \n",
    "        pairwise_diff = high_conf_neg.unsqueeze(0) - low_tpr_pos.unsqueeze(1) + 1.0\n",
    "        pairwise_loss = torch.clamp(pairwise_diff, min=0).mean()\n",
    "    \n",
    "        total_loss = bce_loss + self.lambda_rank * pairwise_loss\n",
    "        return total_loss\n",
    "\n",
    "class PartialAUCWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=1.0, lambda_conf=0.1, alpha=0.2, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))\n",
    "        self.lambda_conf = lambda_conf\n",
    "        self.alpha = alpha\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        # BCE Loss (standard)\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "\n",
    "        # Separate positive and negative examples\n",
    "        pos_mask = targets == 1\n",
    "        neg_mask = targets == 0\n",
    "\n",
    "        pos_scores = logits[pos_mask]\n",
    "        neg_scores = logits[neg_mask]\n",
    "\n",
    "        # Rank negatives to get top α quantile (lowest FPR region)\n",
    "        if len(neg_scores) == 0 or len(pos_scores) == 0:\n",
    "            return bce_loss  # skip partial AUC if not enough data\n",
    "\n",
    "        k = max(1, int(self.alpha * len(neg_scores)))\n",
    "        top_neg_scores, _ = torch.topk(neg_scores, k=k, largest=False)  # lowest scores → high confidence negatives\n",
    "\n",
    "        # Pairwise ranking loss: Hinge loss (1 + neg - pos)_+\n",
    "        # All combinations between pos and top neg\n",
    "        pos_scores = pos_scores.unsqueeze(1)  # shape (P, 1)\n",
    "        top_neg_scores = top_neg_scores.unsqueeze(0)  # shape (1, N)\n",
    "        pairwise_margin = 1.0\n",
    "\n",
    "        pairwise_diff = top_neg_scores - pos_scores + pairwise_margin\n",
    "        pairwise_loss = torch.clamp(pairwise_diff, min=0).mean()\n",
    "\n",
    "        total_loss = bce_loss + self.lambda_conf * pairwise_loss\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return sigmoid_focal_loss(\n",
    "            inputs=inputs,                  \n",
    "            targets=targets.float(),        \n",
    "            alpha=self.alpha,\n",
    "            gamma=self.gamma,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "# params\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AUG = False\n",
    "LORA = True\n",
    "FREEZE = True # whether or not to freeze the weights\n",
    "EPOCHS = 10 # number of training epochs\n",
    "DUPE = True\n",
    "DUPE_FACTOR = 3\n",
    "DUPE_REAL = 25\n",
    "BAL = False\n",
    "REG_DUPE = 100\n",
    "\n",
    "#TRAIN_DIR = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train_images_hair_removed_dullrazor\\\\'\n",
    "TRAIN_CSV = './og_train_dataset_with_labeled_cancer_and_skin_tone.csv' #'./new_train_dataset_with_skin_tone.csv' #\n",
    "TRAIN_DIR = ['./ISIC_2024_Training_Input/ISIC_2024_Training_Input/','./ISIC_2024_Training_Input/ISIC_2024_Training_Input/']#,\"C:\\\\Users\\\\rngki\\\\Downloads\\\\train_images_hair_removed_dullrazor\\\\\"]\n",
    "AUG_DIR = None #'./cleaned_styled_data/cleaned_styled_data/' #FIX\n",
    "AUG_CSV = None #'./cleaned_augmented_data_with_labeled_cancer.csv' #FIX\n",
    "#TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train-metadata.csv'\n",
    "#TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\cleaned_styled_data\\\\cleaned_styled_data\\\\cleaned_augmented_data.csv'\n",
    "#TEST_DIR = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_images_hair_removed_dullrazor\\\\'\n",
    "#TEST_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_dataset_with_skin_tone.csv'\n",
    "BAL_FLAGS = [False,False]\n",
    "CRIT = [HighTPRPartialAUCLoss(device=device),HighTPRPartialAUCLoss(device=device)]\n",
    "#FocalLoss(alpha=0.75, gamma=2.0, reduction='mean').to(device)\n",
    "\n",
    "MODELS = [\"deit\"]\n",
    "#this should be ISIC_2024_Training_Input_std_LORA_FP_.75\n",
    "DATA_NAMES = ['std_Fixed_pAUC','std_nolora_fixed_pAUC']# 'ISIC_2024_Training_Input_std_smart_balance', 'ISIC_2024_Training_Input_std_smart_balance', 'ISIC_2024_Training_Input_std_pAUC', 'ISIC_2024_Training_Input_std_Normal_deit']\n",
    "TEST_DIR = ['./ISIC_2024_Training_Input/ISIC_2024_Training_Input/', './ISIC_2024_Training_Input/ISIC_2024_Training_Input/']\n",
    "TEST_CSV = './og_test_dataset_with_labeled_cancer_and_skin_tone.csv'#'./new_test_dataset_with_skin_tone.csv' #\n",
    "\n",
    "\"\"\"\n",
    "2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def initialize_model(model_name, num_classes=1):\n",
    "    model = None\n",
    "    \n",
    "    if model_name == \"efficientnet\":\n",
    "        target_modules=[\n",
    "            # final linear\n",
    "            \"classifier.1\",\n",
    "            # a couple of the deepest MBConv depthwise convs:\n",
    "            \"features.6.block.2.conv\",  \n",
    "            \"features.7.block.2.conv\",  \n",
    "        ]\n",
    "        model = torchvision.models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.classifier[1]\n",
    "    \n",
    "    elif model_name == \"resnet34\":\n",
    "        target_modules=[\n",
    "            # final classification\n",
    "            \"fc\",\n",
    "            # pick one conv in each of the last 3 residual stages:\n",
    "            \"layer2.0.conv1\",\n",
    "            \"layer3.1.conv2\",\n",
    "            \"layer4.2.conv2\",\n",
    "        ]\n",
    "        model = torchvision.models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.fc\n",
    "    \n",
    "    elif model_name == \"resnet50\":\n",
    "        target_modules=[\n",
    "            # final classification\n",
    "            \"fc\",\n",
    "            # pick one conv in each of the last 3 residual stages:\n",
    "            \"layer2.0.conv1\",\n",
    "            \"layer3.1.conv2\",\n",
    "            \"layer4.2.conv2\",\n",
    "        ]\n",
    "        model = torchvision.models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.fc\n",
    "    \n",
    "    elif model_name == \"coatnet\":\n",
    "        target_modules=[\n",
    "            # final linear\n",
    "            \"head.fc\",\n",
    "            # point‑wise & depth‑wise conv in the last stage:\n",
    "            \"stages.3.blocks.2.pwconv\",\n",
    "            \"stages.3.blocks.2.conv\",  \n",
    "        ]\n",
    "        model = timm.create_model('coatnet_0_rw_224', pretrained=True)\n",
    "        num_ftrs = model.head.fc.in_features\n",
    "        model.head.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.head.fc\n",
    "    \n",
    "    elif model_name == \"deit\":\n",
    "        target_modules=[\n",
    "            # QKV + output projection in each of the first few heads:\n",
    "            \"blocks.0.attn.qkv\",   \"blocks.0.attn.proj\",\n",
    "            \"blocks.1.attn.qkv\",   \"blocks.1.attn.proj\",\n",
    "            # …repeat for as many blocks as you like…\n",
    "            # final classifier(s):\n",
    "            \"head\", \n",
    "            \"head_dist\",         # only for the distilled variant\n",
    "        ]\n",
    "        model = timm.create_model('deit_base_patch16_224', pretrained=True)\n",
    "        num_ftrs = model.head.in_features\n",
    "        model.head = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.head\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "    \n",
    "    # Freeze all layers\n",
    "    if FREEZE:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze only the final classification layer\n",
    "        for param in final_layer.parameters():\n",
    "            param.requires_grad = True\n",
    "    if LORA:\n",
    "        lora_config = LoraConfig(\n",
    "            r=1,\n",
    "            lora_alpha=2,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=target_modules,\n",
    "        )\n",
    "    \n",
    "        model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80) -> float:\n",
    "    '''\n",
    "    2024 ISIC Challenge metric: pAUC\n",
    "    \n",
    "    Given a solution file and submission file, this function returns the\n",
    "    the partial area under the receiver operating characteristic (pAUC) \n",
    "    above a given true positive rate (TPR) = 0.80.\n",
    "    https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "    \n",
    "    (c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\n",
    "    Args:\n",
    "        solution: ground truth pd.DataFrame of 1s and 0s\n",
    "        submission: solution dataframe of predictions of scores ranging [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        Float value range [0, max_fpr]\n",
    "    '''\n",
    "    for col in solution.columns:\n",
    "        if col != 'is_malignant':\n",
    "            del solution[col]\n",
    "    \n",
    "    for col in submission.columns:\n",
    "        if col != 'prediction':\n",
    "            del submission[col]\n",
    "\n",
    "    # check submission is numeric\n",
    "    if not pandas.api.types.is_numeric_dtype(submission.values):\n",
    "        raise ParticipantVisibleError('Submission target column must be numeric')\n",
    "\n",
    "    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)\n",
    "    v_gt = abs(solution.values.ravel()-1)\n",
    "    \n",
    "    # flip the submissions to their compliments\n",
    "    v_pred = -1.0*submission.values.ravel()\n",
    "\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "\n",
    "    # using sklearn.metric functions: (1) roc_curve and (2) auc\n",
    "    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n",
    "    if max_fpr is None or max_fpr == 1:\n",
    "        return auc(fpr, tpr)\n",
    "    if max_fpr <= 0 or max_fpr > 1:\n",
    "        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n",
    "        \n",
    "    # Add a single point at max_fpr by linear interpolation\n",
    "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "    fpr = np.append(fpr[:stop], max_fpr)\n",
    "    partial_auc = auc(fpr, tpr)\n",
    "\n",
    "#     # Equivalent code that uses sklearn's roc_auc_score\n",
    "#     v_gt = abs(np.asarray(solution.values)-1)\n",
    "#     v_pred = np.array([1.0 - x for x in submission.values])\n",
    "#     max_fpr = abs(1-min_tpr)\n",
    "#     partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "#     # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "#     # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "#     partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return(partial_auc)\n",
    "\n",
    "for dir in range(len(TRAIN_DIR)):\n",
    "    BAL = BAL_FLAGS[dir]\n",
    "    USE_SPLIT = False\n",
    "\n",
    "    MALIGNANT = 'target'\n",
    "    # MALIGNANT = 'target'\n",
    "    MALIG_IDX = 1\n",
    "    # MALIG_IDX = 3\n",
    "\n",
    "    if not TEST_DIR:\n",
    "        TEST_DIR = TRAIN_DIR[dir]\n",
    "        USE_SPLIT = True\n",
    "    if not TEST_CSV:\n",
    "        TEST_CSV = TRAIN_CSV\n",
    "        USE_SPLIT = True\n",
    "\n",
    "    # def balance_classes(df, label_col=MALIGNANT):\n",
    "    #     # Separate malignant and non-malignant samples\n",
    "    #     malig_df = df[df[label_col] == 1]\n",
    "    #     non_malig_df = df[df[label_col] == 0]\n",
    "\n",
    "    #     # Get the minority count\n",
    "    #     min_count = min(len(malig_df), len(non_malig_df))\n",
    "\n",
    "    #     # Downsample both classes to min_count (or oversample malignant if needed)\n",
    "    #     non_malig_df_balanced = non_malig_df.sample(n=min_count, random_state=42)\n",
    "    #     malig_df_balanced = malig_df.sample(n=min_count, replace=True, random_state=42)  # oversample if needed\n",
    "\n",
    "    #     # Concatenate back\n",
    "    #     balanced_df = pd.concat([malig_df_balanced, non_malig_df_balanced]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    #     return balanced_df\n",
    "\n",
    "    if USE_SPLIT:\n",
    "        df = pd.read_csv(TRAIN_CSV)\n",
    "        #df = pd.read_csv('C:\\\\Users\\\\rngki\\\\Downloads\\\\cleaned_styled_data\\\\cleaned_styled_data\\\\cleaned_augmented_data.csv')\n",
    "        print(f\"num malignant: {sum(df[MALIGNANT])}\")\n",
    "\n",
    "        # downsample for time\n",
    "        df = df.sample(frac=.1, random_state=42)\n",
    "\n",
    "        train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[MALIGNANT], random_state=42)\n",
    "    elif AUG:\n",
    "        OG_train_df = pd.read_csv(TRAIN_CSV)\n",
    "        AUG_train = pd.read_csv(AUG_CSV)\n",
    "        # Add a source column to each\n",
    "        OG_train_df[\"source\"] = 0\n",
    "        AUG_train[\"source\"] = 1\n",
    "        \n",
    "        #SOURCE_IDX = AUG_train.columns.get_loc(\"source\")\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "        train_df = pd.concat([OG_train_df, AUG_train], ignore_index=True)\n",
    "        val_df = pd.read_csv(TEST_CSV)\n",
    "    else:\n",
    "        train_df = pd.read_csv(TRAIN_CSV)\n",
    "        val_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "    if DUPE:\n",
    "\n",
    "        if AUG:\n",
    "            real_malig_df = train_df[(train_df[MALIGNANT] == 1) & (train_df['source'] == 0)]\n",
    "            fake_malig_df = train_df[(train_df[MALIGNANT] == 1) & (train_df['source'] == 1)]\n",
    "            \n",
    "            real_malig_df = pd.concat([real_malig_df]*DUPE_REAL*DUPE_FACTOR, ignore_index=True)\n",
    "            fake_malig_df = pd.concat([fake_malig_df]*DUPE_FACTOR, ignore_index=True)\n",
    "    \n",
    "            train_df = pd.concat([train_df, real_malig_df,fake_malig_df], ignore_index=True)\n",
    "    \n",
    "            if BAL:\n",
    "                # Step 1: Separate malignant and non-malignant\n",
    "                malig_df = train_df[train_df[MALIGNANT] == 1]\n",
    "                \n",
    "                non_malig_df = train_df[train_df[MALIGNANT] == 0]\n",
    "                \n",
    "                # Step 2: Balance malignant skin tones within each source\n",
    "                balanced_by_source = []\n",
    "                for src in [0, 1]:\n",
    "                    sub_df = malig_df[malig_df[\"source\"] == src]\n",
    "                \n",
    "                    if sub_df.empty:\n",
    "    \n",
    "                        print(\"problem\")\n",
    "                        continue  # Skip if no data for this source\n",
    "                \n",
    "                    # Get max count for balancing\n",
    "                    skin_counts = sub_df[\"skin_tone\"].value_counts()\n",
    "                    max_count = skin_counts.max()\n",
    "                \n",
    "                    # Duplicate each skin tone group to match max_count\n",
    "                    balanced_sub = (\n",
    "                        sub_df.groupby(\"skin_tone\", group_keys=False)\n",
    "                        .apply(lambda x: x.sample(max_count, replace=True))\n",
    "                    )\n",
    "                    balanced_by_source.append(balanced_sub)\n",
    "                \n",
    "                # Step 3: Combine balanced malignant data\n",
    "                malig_df_balanced = pd.concat(balanced_by_source).reset_index(drop=True)\n",
    "                \n",
    "                # Step 4: Combine with non-malignant data and shuffle\n",
    "                train_df= pd.concat([non_malig_df, malig_df_balanced], ignore_index=True)\n",
    "                train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            malig_df = train_df[train_df[MALIGNANT] == 1]\n",
    "            malig_df = pd.concat([malig_df]*REG_DUPE, ignore_index=True)\n",
    "            \n",
    "            train_df = pd.concat([train_df, malig_df], ignore_index=True)\n",
    "            train_df[\"source\"] = 0\n",
    "            if BAL:\n",
    "                # Step 1: Separate malignant and non-malignant\n",
    "                malig_df = train_df[train_df[MALIGNANT] == 1]\n",
    "                \n",
    "                non_malig_df = train_df[train_df[MALIGNANT] == 0]\n",
    "                \n",
    "                # Step 2: Balance malignant skin tones within each source\n",
    "                balanced_by_source = []\n",
    "                for src in [0, 1]:\n",
    "                    sub_df = malig_df[malig_df[\"source\"] == src]\n",
    "                \n",
    "                    if sub_df.empty:\n",
    "    \n",
    "                        print(\"problem\")\n",
    "                        continue  # Skip if no data for this source\n",
    "                \n",
    "                    # Get max count for balancing\n",
    "                    skin_counts = sub_df[\"skin_tone\"].value_counts()\n",
    "                    max_count = skin_counts.max()\n",
    "                \n",
    "                    # Duplicate each skin tone group to match max_count\n",
    "                    balanced_sub = (\n",
    "                        sub_df.groupby(\"skin_tone\", group_keys=False)\n",
    "                        .apply(lambda x: x.sample(max_count, replace=True))\n",
    "                    )\n",
    "                    balanced_by_source.append(balanced_sub)\n",
    "                \n",
    "                # Step 3: Combine balanced malignant data\n",
    "                malig_df_balanced = pd.concat(balanced_by_source).reset_index(drop=True)\n",
    "                \n",
    "                # Step 4: Combine with non-malignant data and shuffle\n",
    "                train_df= pd.concat([non_malig_df, malig_df_balanced], ignore_index=True)\n",
    "                train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    train_df.to_csv('train_labels.csv', index=False)\n",
    "    val_df.to_csv('val_labels.csv', index=False)\n",
    "\n",
    "    print(f\"Validation - Num malignant: {val_df[MALIGNANT].sum()}\")\n",
    "    print(f\"Validation - Num benign: {len(val_df) - val_df[MALIGNANT].sum()}\")\n",
    "    print(f\"Training - Num malignant: {train_df[MALIGNANT].sum()}\")\n",
    "    print(f\"Training - Num benign: {len(train_df) - train_df[MALIGNANT].sum()}\")\n",
    "\n",
    "    class ISICDataset(Dataset):\n",
    "        def __init__(self, csv_file, img_dir, non_malignant_transform=None, malignant_transform=None,label_col=MALIGNANT,aug_dir = None, train_bool = False,Balance= False):\n",
    "            self.fulldata = pd.read_csv(csv_file)\n",
    "            self.train = train_bool\n",
    "            self.balanced = Balance\n",
    "            if self.train:\n",
    "                self.malig_df = self.fulldata[self.fulldata[label_col] ==1]\n",
    "                self.non_malig_df = self.fulldata[self.fulldata[label_col] ==0]\n",
    "                self.min_count = min(len(self.malig_df), len(self.non_malig_df))\n",
    "                non_malig_df_balanced = self.non_malig_df.sample(n=self.min_count)\n",
    "\n",
    "                # Concatenate back\n",
    "                self.df = pd.concat([self.malig_df, non_malig_df_balanced]).sample(frac=1).reset_index(drop=True)\n",
    "                if aug_dir != None:\n",
    "                    self.SOURCE_IDX = self.df.columns.get_loc(\"source\")\n",
    "            else:\n",
    "                self.df = self.fulldata\n",
    "\n",
    "            if self.balanced:\n",
    "                self.skin_idx = self.df.columns.get_loc(\"skin_tone\")\n",
    "            \n",
    "            self.img_dir = img_dir\n",
    "            self.aug_dir = aug_dir\n",
    "            self.non_malignant_transform = non_malignant_transform\n",
    "            self.malignant_transform = malignant_transform\n",
    "\n",
    "        def rebalance(self):\n",
    "            if not self.train:\n",
    "                return\n",
    "            \n",
    "            if (self.balanced):\n",
    "                malig_skin_tones = set(self.malig_df[\"skin_tone\"].unique())\n",
    "                nonmalig_skin_tones = set(self.non_malig_df[\"skin_tone\"].unique())\n",
    "                \n",
    "                exclusive_nonmalig_skin_tones = nonmalig_skin_tones - malig_skin_tones\n",
    "                extra_sample_frac = 0.05 # 5% of total non_malig_df\n",
    "\n",
    "                # Filter only the exclusive skin tones\n",
    "                nonmalig_extras = self.non_malig_df[self.non_malig_df[\"skin_tone\"].isin(exclusive_nonmalig_skin_tones)]\n",
    "                \n",
    "                # Sample a small subset\n",
    "                nonmalig_extras_sampled = nonmalig_extras.sample(int(extra_sample_frac*self.min_count), replace=True)\n",
    "                \n",
    "                # Get skin_tone distribution in malignant data\n",
    "                malig_skin_dist = self.malig_df[\"skin_tone\"].value_counts(normalize=True)\n",
    "\n",
    "                # Sample from non-malignant data using the same proportions\n",
    "                non_malig_df_balanced = (\n",
    "                    self.non_malig_df.groupby(\"skin_tone\", group_keys=False)\n",
    "                    .apply(lambda x: x.sample(int((1-extra_sample_frac)*self.min_count * malig_skin_dist.get(x.name, 0)), replace=True))\n",
    "                )\n",
    "\n",
    "                # Combine and shuffle\n",
    "                self.df = pd.concat([self.malig_df, non_malig_df_balanced]).sample(frac=1).reset_index(drop=True)\n",
    "            else:\n",
    "                non_malig_df_balanced = self.non_malig_df.sample(n=self.min_count)\n",
    "                # Concatenate back\n",
    "                self.df = pd.concat([self.malig_df, non_malig_df_balanced]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if(self.aug_dir == None):\n",
    "                img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "                    image = self.non_malignant_transform(image)\n",
    "                elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "                    image = self.malignant_transform(image)\n",
    "\n",
    "                return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0]\n",
    "            else:\n",
    "                if(self.df.iloc[idx, self.SOURCE_IDX] == 0):\n",
    "\n",
    "                    img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                    if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "                        image = self.non_malignant_transform(image)\n",
    "                    elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "                        image = self.malignant_transform(image)\n",
    "\n",
    "                    return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0] \n",
    "                else:\n",
    "                    img_path = os.path.join(self.aug_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                    if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "                        image = self.non_malignant_transform(image)\n",
    "                    elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "                        image = self.malignant_transform(image)\n",
    "\n",
    "                    return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0] \n",
    "\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "                            std=[0.229, 0.224, 0.225])    # ImageNet stds\n",
    "    ])\n",
    "\n",
    "    # malignant_transform = transforms.Compose([\n",
    "    #     transforms.Resize((224, 224)),\n",
    "    #     transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #     transforms.RandomVerticalFlip(p=0.5),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "    malignant_transform = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomRotation(degrees=360),  # Rotate by a random degree\n",
    "        transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),  # Random crop and resize to 224x224\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    val_dataset = ISICDataset(\n",
    "        csv_file='val_labels.csv',\n",
    "        img_dir=TEST_DIR[dir],\n",
    "        non_malignant_transform=transform,\n",
    "        malignant_transform=transform\n",
    "    )\n",
    "\n",
    "    train_dataset = ISICDataset(\n",
    "        csv_file='train_labels.csv',\n",
    "        img_dir=TRAIN_DIR[dir],\n",
    "        non_malignant_transform=malignant_transform,\n",
    "        malignant_transform=malignant_transform,\n",
    "        aug_dir = AUG_DIR,\n",
    "        train_bool = True,\n",
    "        Balance= BAL\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"Training on {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "    for model_name in MODELS:\n",
    "        resnet = initialize_model(model_name=model_name, num_classes=1)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=1e-4)\n",
    "        criterion = CRIT[dir]\n",
    "        #pos_weight = torch.tensor([1.0]).to(device)\n",
    "        #criterion = ConfidentBCEWithLogitsLoss(pos_weight=pos_weight, lambda_conf=0.1, device=device)\n",
    "        #criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0]).to(device))\n",
    "        #criterion = PartialAUCWithLogitsLoss(pos_weight=5, lambda_conf=1, device='cuda')\n",
    "\n",
    "        # switch to this if we're doing more than malignant/not-malignant\n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        resnet.to(device)\n",
    "\n",
    "        with open(f\"training_log_{model_name}_{DATA_NAMES[dir]}_{dir}.csv\", \"w\") as f:\n",
    "            f.write(\"epoch,train_loss,val_loss,val_tn,val_fn,val_tp,val_fp,val_accuracy,val_auc,pauc,f1score\\n\")\n",
    "            start_time = time.time()\n",
    "            for epoch in range(EPOCHS):\n",
    "                train_dataset.rebalance()\n",
    "                dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "                valloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "                print(f\"Training set size: {len(dataloader.dataset)}\")\n",
    "                print(f\"Validation set size: {len(valloader.dataset)}\")\n",
    "\n",
    "                resnet.train()\n",
    "                avgloss = 0.0\n",
    "                print(f\"EPOCH: {epoch + 1}\")\n",
    "                for images, labels, _ in dataloader:\n",
    "                    images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = resnet(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    avgloss += loss.item() * images.size(0)\n",
    "                avgloss /= len(dataloader.dataset)\n",
    "\n",
    "                resnet.eval()\n",
    "                val_loss = 0.0\n",
    "                total = 0\n",
    "                false_negative = 0\n",
    "                false_positive = 0\n",
    "                true_negative = 0\n",
    "                true_positive = 0\n",
    "                all_labels = []\n",
    "                all_probs = []  # Store probabilities instead of binary predictions\n",
    "                image_ids_list = []  # Collect image IDs for submission\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for images, labels, image_ids in valloader:  # Ensure dataset returns image IDs\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device).float().unsqueeze(1)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        outputs = resnet(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                        # Get probabilities (before thresholding)\n",
    "                        probabilities = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                        all_probs.extend(probabilities)\n",
    "                        \n",
    "                        # Get binary predictions for confusion matrix\n",
    "                        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                        preds = preds.cpu().numpy().flatten()\n",
    "                        \n",
    "                        # Track image IDs for submission\n",
    "                        image_ids_list.extend(image_ids)  # Assuming image_ids are strings\n",
    "                        \n",
    "                        # Update confusion matrix\n",
    "                        for p, l in zip(preds, labels.cpu().numpy()):\n",
    "                            if p == 0 and l == 0:\n",
    "                                true_negative += 1\n",
    "                            elif p == 0 and l == 1:\n",
    "                                false_negative += 1\n",
    "                            elif p == 1 and l == 0:\n",
    "                                false_positive += 1\n",
    "                            elif p == 1 and l == 1:\n",
    "                                true_positive += 1\n",
    "                        total += labels.size(0)\n",
    "                        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "                # Calculate metrics\n",
    "                avg_val_loss = val_loss / len(valloader.dataset)\n",
    "                accuracy = (true_positive + true_negative) / total\n",
    "                p_auc = roc_auc_score(all_labels, all_probs)  # Use probabilities for AUC\n",
    "                f1score = f1_score(all_labels, [1 if x > 0.5 else 0 for x in all_probs])\n",
    "\n",
    "                # Create submission DataFrame\n",
    "                submission_df = pd.DataFrame({\n",
    "                    'isic_id': image_ids_list,\n",
    "                    'prediction': all_probs\n",
    "                })\n",
    "\n",
    "                solution_df = pd.DataFrame({\n",
    "                    'isic_id': image_ids_list,\n",
    "                    'is_malignant': all_labels\n",
    "                })\n",
    "\n",
    "                # Calculate pAUC using the competition metric\n",
    "                try:\n",
    "                    pAUC = score(\n",
    "                        solution=solution_df,\n",
    "                        submission=submission_df,\n",
    "                        row_id_column_name='isic_id'  # Must match your column name\n",
    "                    )\n",
    "                except ParticipantVisibleError as e:\n",
    "                    print(f\"Scoring Error: {e}\")\n",
    "                    pAUC = -1  # Handle invalid submissions\n",
    "\n",
    "                if (epoch+1) % 5 == 0:\n",
    "                    torch.save(resnet.state_dict(), f\"resnet50_{epoch}_{DUPE}_{FREEZE}_pos_weight(2).pth\")\n",
    "                \n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "                    f\"Train Loss: {avgloss:.4f} \"\n",
    "                    f\"Val Loss: {avg_val_loss:.4f}\\n\"\n",
    "                    f\"Val TN: {true_negative} FN: {false_negative} \\n\"\n",
    "                    f\"TP: {true_positive} FP: {false_positive}\\n\"\n",
    "                    f\"Val Accuracy: {accuracy:.4f}\\n\"\n",
    "                    f\"AUC: {p_auc:.4f}\\n\"\n",
    "                    f\"pAUC: {pAUC:.4f}\\n\"\n",
    "                    f\"f1 score: {f1score:.4f}\\n\")  # Add pAUC to output'\n",
    "                f.write(f\"{epoch+1},{avgloss},{avg_val_loss},{true_negative},{false_negative},{true_positive},{false_positive},{accuracy},{p_auc},{pAUC},{f1score}\\n\")\n",
    "                with open(f\"training_log_{model_name}_{DATA_NAMES[dir]}_{dir}live.csv\", \"w\") as f1:\n",
    "                    f1.write(f\"{epoch+1},{avgloss},{avg_val_loss},{true_negative},{false_negative},{true_positive},{false_positive},{accuracy},{p_auc},{pAUC},{f1score}\\n\")\n",
    "                with open(f\"scores_{model_name}_{DATA_NAMES[dir]}_{dir}.csv\", \"w\") as f1:\n",
    "                    f1.write(\"isic_id,prediction,target\\n\")\n",
    "                    for id, fscore, actual in zip(image_ids_list, all_probs, all_labels):\n",
    "                        f1.write(f\"{id},{fscore},{actual}\\n\")\n",
    "            \n",
    "        print(f\"Training time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        with open(f\"scores_{model_name}_{DATA_NAMES[dir]}_{dir}.csv\", \"w\") as f:\n",
    "            f.write(\"isic_id,prediction,target\\n\")\n",
    "            for id, fscore, actual in zip(image_ids_list, all_probs, all_labels):\n",
    "                f.write(f\"{id},{fscore},{actual}\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        def plot_roc_and_pauc(y_true, y_scores, min_tpr=0.80):\n",
    "            # Flip labels and predictions (based on your scoring function)\n",
    "            v_gt = abs(np.asarray(y_true) - 1)\n",
    "            v_pred = -1.0 * np.asarray(y_scores)\n",
    "\n",
    "            # Calculate ROC curve\n",
    "            fpr, tpr, thresholds = roc_curve(v_gt, v_pred)\n",
    "\n",
    "            # Compute full AUC and partial AUC\n",
    "            full_auc = auc(fpr, tpr)\n",
    "            max_fpr = abs(1 - min_tpr)\n",
    "\n",
    "            # Partial AUC calculation (manual interpolation for the cutoff point)\n",
    "            stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "            x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "            y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "            \n",
    "            # Interpolated TPR at max_fpr\n",
    "            interp_tpr = np.interp(max_fpr, x_interp, y_interp)\n",
    "            \n",
    "            # Create partial ROC arrays up to max_fpr\n",
    "            pauc_fpr = np.append(fpr[:stop], max_fpr)\n",
    "            pauc_tpr = np.append(tpr[:stop], interp_tpr)\n",
    "            \n",
    "            partial_auc = auc(pauc_fpr, pauc_tpr)\n",
    "\n",
    "            # Plotting\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {full_auc:.4f})\")\n",
    "            \n",
    "            # Shade pAUC region\n",
    "            plt.fill_between(pauc_fpr, pauc_tpr, step=\"post\", alpha=0.3, color=\"orange\", label=f\"pAUC = {partial_auc:.4f}\")\n",
    "            \n",
    "            # Draw horizontal line at TPR = min_tpr\n",
    "            plt.axhline(min_tpr, color='red', linestyle='--', label=f\"Min TPR = {min_tpr}\")\n",
    "\n",
    "            # Draw vertical line at FPR = max_fpr\n",
    "            plt.axvline(max_fpr, color='green', linestyle='--', label=f\"Max FPR = {max_fpr:.2f}\")\n",
    "\n",
    "            plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "            plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "            plt.title(\"ROC Curve with pAUC Region Highlighted\")\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"roc_curve_{model_name}_{DATA_NAMES[dir]}_{dir}.png\")\n",
    "\n",
    "        # Example usage\n",
    "        plot_roc_and_pauc(all_labels, all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import torchvision\n",
    "import timm\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return sigmoid_focal_loss(\n",
    "            inputs=inputs,                  \n",
    "            targets=targets.float(),        \n",
    "            alpha=self.alpha,\n",
    "            gamma=self.gamma,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "\n",
    "# params\n",
    "AUG = True\n",
    "LORA = True\n",
    "FREEZE = True # whether or not to freeze the weights\n",
    "EPOCHS = 15 # number of training epochs\n",
    "DUPE = True\n",
    "DUPE_FACTOR = 4\n",
    "DUPE_REAL = 12\n",
    "BAL = False\n",
    "#TRAIN_DIR = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train_images_hair_removed_dullrazor\\\\'\n",
    "TRAIN_CSV = './og_train_dataset_with_labeled_cancer_and_skin_tone.csv'\n",
    "TRAIN_DIR = ['./ISIC_2024_Training_Input/ISIC_2024_Training_Input/'] #,\"C:\\\\Users\\\\rngki\\\\Downloads\\\\train_images_hair_removed_dullrazor\\\\\"]\n",
    "AUG_DIR = './cleaned_styled_data/cleaned_styled_data/' #FIX\n",
    "AUG_CSV = './cleaned_augmented_data_with_labeled_cancer.csv' #FIX\n",
    "#TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train-metadata.csv'\n",
    "#TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\cleaned_styled_data\\\\cleaned_styled_data\\\\cleaned_augmented_data.csv'\n",
    "#TEST_DIR = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_images_hair_removed_dullrazor\\\\'\n",
    "#TEST_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_dataset_with_skin_tone.csv'\n",
    "\n",
    "MODELS = [\"deit\"]\n",
    "\n",
    "DATA_NAMES = ['ISIC_2024_Training_Input_ST_LORA_DEIT_FOCAL9']\n",
    "TEST_DIR = ['./ISIC_2024_Training_Input/ISIC_2024_Training_Input/']\n",
    "TEST_CSV = './og_test_dataset_with_labeled_cancer_and_skin_tone.csv'\n",
    "\n",
    "\"\"\"\n",
    "2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def initialize_model(model_name, num_classes=1):\n",
    "    model = None\n",
    "    \n",
    "    if model_name == \"efficientnet\":\n",
    "        target_modules=[\n",
    "            # final linear\n",
    "            \"classifier.1\",\n",
    "            # a couple of the deepest MBConv depthwise convs:\n",
    "            \"features.6.block.2.conv\",  \n",
    "            \"features.7.block.2.conv\",  \n",
    "        ]\n",
    "        model = torchvision.models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.classifier[1]\n",
    "    \n",
    "    elif model_name == \"resnet34\":\n",
    "        target_modules=[\n",
    "            # final classification\n",
    "            \"fc\",\n",
    "            # pick one conv in each of the last 3 residual stages:\n",
    "            \"layer2.0.conv1\",\n",
    "            \"layer3.1.conv2\",\n",
    "            \"layer4.2.conv2\",\n",
    "        ]\n",
    "        model = torchvision.models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.fc\n",
    "    \n",
    "    elif model_name == \"resnet50\":\n",
    "        target_modules=[\n",
    "            # final classification\n",
    "            \"fc\",\n",
    "            # pick one conv in each of the last 3 residual stages:\n",
    "            \"layer2.0.conv1\",\n",
    "            \"layer3.1.conv2\",\n",
    "            \"layer4.2.conv2\",\n",
    "        ]\n",
    "        model = torchvision.models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.fc\n",
    "    \n",
    "    elif model_name == \"coatnet\":\n",
    "        target_modules=[\n",
    "            # final linear\n",
    "            \"head.fc\",\n",
    "            # point‑wise & depth‑wise conv in the last stage:\n",
    "            \"stages.3.blocks.2.pwconv\",\n",
    "            \"stages.3.blocks.2.conv\",  \n",
    "        ]\n",
    "        model = timm.create_model('coatnet_0_rw_224', pretrained=True)\n",
    "        num_ftrs = model.head.fc.in_features\n",
    "        model.head.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.head.fc\n",
    "    \n",
    "    elif model_name == \"deit\":\n",
    "        target_modules=[\n",
    "            # QKV + output projection in each of the first few heads:\n",
    "            \"blocks.0.attn.qkv\",   \"blocks.0.attn.proj\",\n",
    "            \"blocks.1.attn.qkv\",   \"blocks.1.attn.proj\",\n",
    "            # …repeat for as many blocks as you like…\n",
    "            # final classifier(s):\n",
    "            \"head\", \n",
    "            \"head_dist\",         # only for the distilled variant\n",
    "        ]\n",
    "        model = timm.create_model('deit_base_patch16_224', pretrained=True)\n",
    "        num_ftrs = model.head.in_features\n",
    "        model.head = nn.Linear(num_ftrs, num_classes)\n",
    "        final_layer = model.head\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "    \n",
    "    # Freeze all layers\n",
    "    if FREEZE:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze only the final classification layer\n",
    "        for param in final_layer.parameters():\n",
    "            param.requires_grad = True\n",
    "    if LORA:\n",
    "        lora_config = LoraConfig(\n",
    "            r=1,\n",
    "            lora_alpha=2,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=target_modules,\n",
    "        )\n",
    "    \n",
    "        model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80) -> float:\n",
    "    '''\n",
    "    2024 ISIC Challenge metric: pAUC\n",
    "    \n",
    "    Given a solution file and submission file, this function returns the\n",
    "    the partial area under the receiver operating characteristic (pAUC) \n",
    "    above a given true positive rate (TPR) = 0.80.\n",
    "    https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "    \n",
    "    (c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\n",
    "    Args:\n",
    "        solution: ground truth pd.DataFrame of 1s and 0s\n",
    "        submission: solution dataframe of predictions of scores ranging [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        Float value range [0, max_fpr]\n",
    "    '''\n",
    "    for col in solution.columns:\n",
    "        if col != 'is_malignant':\n",
    "            del solution[col]\n",
    "    \n",
    "    for col in submission.columns:\n",
    "        if col != 'prediction':\n",
    "            del submission[col]\n",
    "\n",
    "    # check submission is numeric\n",
    "    if not pandas.api.types.is_numeric_dtype(submission.values):\n",
    "        raise ParticipantVisibleError('Submission target column must be numeric')\n",
    "\n",
    "    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)\n",
    "    v_gt = abs(solution.values.ravel()-1)\n",
    "    \n",
    "    # flip the submissions to their compliments\n",
    "    v_pred = -1.0*submission.values.ravel()\n",
    "\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "\n",
    "    # using sklearn.metric functions: (1) roc_curve and (2) auc\n",
    "    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n",
    "    if max_fpr is None or max_fpr == 1:\n",
    "        return auc(fpr, tpr)\n",
    "    if max_fpr <= 0 or max_fpr > 1:\n",
    "        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n",
    "        \n",
    "    # Add a single point at max_fpr by linear interpolation\n",
    "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "    fpr = np.append(fpr[:stop], max_fpr)\n",
    "    partial_auc = auc(fpr, tpr)\n",
    "\n",
    "#     # Equivalent code that uses sklearn's roc_auc_score\n",
    "#     v_gt = abs(np.asarray(solution.values)-1)\n",
    "#     v_pred = np.array([1.0 - x for x in submission.values])\n",
    "#     max_fpr = abs(1-min_tpr)\n",
    "#     partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "#     # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "#     # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "#     partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return(partial_auc)\n",
    "\n",
    "for dir in range(len(TRAIN_DIR)):\n",
    "    USE_SPLIT = False\n",
    "\n",
    "    MALIGNANT = 'target'\n",
    "    # MALIGNANT = 'target'\n",
    "    MALIG_IDX = 1\n",
    "    # MALIG_IDX = 3\n",
    "\n",
    "    if not TEST_DIR:\n",
    "        TEST_DIR = TRAIN_DIR[dir]\n",
    "        USE_SPLIT = True\n",
    "    if not TEST_CSV:\n",
    "        TEST_CSV = TRAIN_CSV\n",
    "        USE_SPLIT = True\n",
    "\n",
    "    # def balance_classes(df, label_col=MALIGNANT):\n",
    "    #     # Separate malignant and non-malignant samples\n",
    "    #     malig_df = df[df[label_col] == 1]\n",
    "    #     non_malig_df = df[df[label_col] == 0]\n",
    "\n",
    "    #     # Get the minority count\n",
    "    #     min_count = min(len(malig_df), len(non_malig_df))\n",
    "\n",
    "    #     # Downsample both classes to min_count (or oversample malignant if needed)\n",
    "    #     non_malig_df_balanced = non_malig_df.sample(n=min_count, random_state=42)\n",
    "    #     malig_df_balanced = malig_df.sample(n=min_count, replace=True, random_state=42)  # oversample if needed\n",
    "\n",
    "    #     # Concatenate back\n",
    "    #     balanced_df = pd.concat([malig_df_balanced, non_malig_df_balanced]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    #     return balanced_df\n",
    "\n",
    "    if USE_SPLIT:\n",
    "        df = pd.read_csv(TRAIN_CSV)\n",
    "        #df = pd.read_csv('C:\\\\Users\\\\rngki\\\\Downloads\\\\cleaned_styled_data\\\\cleaned_styled_data\\\\cleaned_augmented_data.csv')\n",
    "        print(f\"num malignant: {sum(df[MALIGNANT])}\")\n",
    "\n",
    "        # downsample for time\n",
    "        df = df.sample(frac=.1, random_state=42)\n",
    "\n",
    "        train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[MALIGNANT], random_state=42)\n",
    "    elif AUG:\n",
    "        OG_train_df = pd.read_csv(TRAIN_CSV)\n",
    "        AUG_train = pd.read_csv(AUG_CSV)\n",
    "        # Add a source column to each\n",
    "        OG_train_df[\"source\"] = 0\n",
    "        AUG_train[\"source\"] = 1\n",
    "        \n",
    "        #SOURCE_IDX = AUG_train.columns.get_loc(\"source\")\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "        train_df = pd.concat([OG_train_df, AUG_train], ignore_index=True)\n",
    "        val_df = pd.read_csv(TEST_CSV)\n",
    "    else:\n",
    "        train_df = pd.read_csv(TRAIN_CSV)\n",
    "        val_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "    if DUPE:\n",
    "\n",
    "        if AUG:\n",
    "            real_malig_df = train_df[(train_df[MALIGNANT] == 1) & (train_df['source'] == 0)]\n",
    "            fake_malig_df = train_df[(train_df[MALIGNANT] == 1) & (train_df['source'] == 1)]\n",
    "            \n",
    "            real_malig_df = pd.concat([real_malig_df]*DUPE_REAL*DUPE_FACTOR, ignore_index=True)\n",
    "            fake_malig_df = pd.concat([fake_malig_df]*DUPE_FACTOR, ignore_index=True)\n",
    "    \n",
    "            train_df = pd.concat([train_df, real_malig_df,fake_malig_df], ignore_index=True)\n",
    "    \n",
    "            if BAL:\n",
    "                # Step 1: Separate malignant and non-malignant\n",
    "                malig_df = train_df[train_df[MALIGNANT] == 1]\n",
    "                \n",
    "                non_malig_df = train_df[train_df[MALIGNANT] == 0]\n",
    "                \n",
    "                # Step 2: Balance malignant skin tones within each source\n",
    "                balanced_by_source = []\n",
    "                for src in [0, 1]:\n",
    "                    sub_df = malig_df[malig_df[\"source\"] == src]\n",
    "                \n",
    "                    if sub_df.empty:\n",
    "    \n",
    "                        print(\"problem\")\n",
    "                        continue  # Skip if no data for this source\n",
    "                \n",
    "                    # Get max count for balancing\n",
    "                    skin_counts = sub_df[\"skin_tone\"].value_counts()\n",
    "                    max_count = skin_counts.max()\n",
    "                \n",
    "                    # Duplicate each skin tone group to match max_count\n",
    "                    balanced_sub = (\n",
    "                        sub_df.groupby(\"skin_tone\", group_keys=False)\n",
    "                        .apply(lambda x: x.sample(max_count, replace=True))\n",
    "                    )\n",
    "                    balanced_by_source.append(balanced_sub)\n",
    "                \n",
    "                # Step 3: Combine balanced malignant data\n",
    "                malig_df_balanced = pd.concat(balanced_by_source).reset_index(drop=True)\n",
    "                \n",
    "                # Step 4: Combine with non-malignant data and shuffle\n",
    "                train_df= pd.concat([non_malig_df, malig_df_balanced], ignore_index=True)\n",
    "                train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            malig_df = train_df[train_df[MALIGNANT] == 1]\n",
    "            malig_df = pd.concat([malig_df]*100, ignore_index=True)\n",
    "            \n",
    "            train_df = pd.concat([train_df, malig_df], ignore_index=True)\n",
    "            train_df[\"source\"] = 0\n",
    "\n",
    "\n",
    "    train_df.to_csv('train_labels.csv', index=False)\n",
    "    val_df.to_csv('val_labels.csv', index=False)\n",
    "\n",
    "    print(f\"Validation - Num malignant: {val_df[MALIGNANT].sum()}\")\n",
    "    print(f\"Validation - Num benign: {len(val_df) - val_df[MALIGNANT].sum()}\")\n",
    "    print(f\"Training - Num malignant: {train_df[MALIGNANT].sum()}\")\n",
    "    print(f\"Training - Num benign: {len(train_df) - train_df[MALIGNANT].sum()}\")\n",
    "\n",
    "    class ISICDataset(Dataset):\n",
    "        def __init__(self, csv_file, img_dir, non_malignant_transform=None, malignant_transform=None,label_col=MALIGNANT,aug_dir = None, train_bool = False,Balance= False):\n",
    "            self.fulldata = pd.read_csv(csv_file)\n",
    "            self.train = train_bool\n",
    "            self.balanced = Balance\n",
    "            if self.train:\n",
    "                self.malig_df = self.fulldata[self.fulldata[label_col] ==1]\n",
    "                self.non_malig_df = self.fulldata[self.fulldata[label_col] ==0]\n",
    "                self.min_count = min(len(self.malig_df), len(self.non_malig_df))\n",
    "                non_malig_df_balanced = self.non_malig_df.sample(n=self.min_count)\n",
    "\n",
    "                # Concatenate back\n",
    "                self.df = pd.concat([self.malig_df, non_malig_df_balanced]).sample(frac=1).reset_index(drop=True)\n",
    "                if aug_dir != None:\n",
    "                    self.SOURCE_IDX = self.df.columns.get_loc(\"source\")\n",
    "            else:\n",
    "                self.df = self.fulldata\n",
    "\n",
    "            if self.balanced:\n",
    "                self.skin_idx = self.df.columns.get_loc(\"skin_tone\")\n",
    "            \n",
    "            self.img_dir = img_dir\n",
    "            self.aug_dir = aug_dir\n",
    "            self.non_malignant_transform = non_malignant_transform\n",
    "            self.malignant_transform = malignant_transform\n",
    "\n",
    "        def rebalance(self):\n",
    "            if not self.train:\n",
    "                return\n",
    "            \n",
    "            if (self.balanced):\n",
    "                malig_skin_tones = set(self.malig_df[\"skin_tone\"].unique())\n",
    "                nonmalig_skin_tones = set(self.non_malig_df[\"skin_tone\"].unique())\n",
    "                \n",
    "                exclusive_nonmalig_skin_tones = nonmalig_skin_tones - malig_skin_tones\n",
    "                extra_sample_frac = 0.05  # 5% of total non_malig_df\n",
    "\n",
    "                # Filter only the exclusive skin tones\n",
    "                nonmalig_extras = self.non_malig_df[self.non_malig_df[\"skin_tone\"].isin(exclusive_nonmalig_skin_tones)]\n",
    "                \n",
    "                # Sample a small subset\n",
    "                nonmalig_extras_sampled = nonmalig_extras.sample(int(extra_sample_frac*self.min_count), replace=True)\n",
    "                \n",
    "                # Get skin_tone distribution in malignant data\n",
    "                malig_skin_dist = self.malig_df[\"skin_tone\"].value_counts(normalize=True)\n",
    "\n",
    "                # Sample from non-malignant data using the same proportions\n",
    "                non_malig_df_balanced = (\n",
    "                    self.non_malig_df.groupby(\"skin_tone\", group_keys=False)\n",
    "                    .apply(lambda x: x.sample(int((1-extra_sample_frac)*self.min_count * malig_skin_dist.get(x.name, 0)), replace=True))\n",
    "                )\n",
    "\n",
    "                # Combine and shuffle\n",
    "                self.df = pd.concat([self.malig_df, non_malig_df_balanced]).sample(frac=1).reset_index(drop=True)\n",
    "            else:\n",
    "                non_malig_df_balanced = self.non_malig_df.sample(n=self.min_count)\n",
    "                # Concatenate back\n",
    "                self.df = pd.concat([self.malig_df, non_malig_df_balanced]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if(self.aug_dir == None):\n",
    "                img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "                    image = self.non_malignant_transform(image)\n",
    "                elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "                    image = self.malignant_transform(image)\n",
    "\n",
    "                return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0]\n",
    "            else:\n",
    "                if(self.df.iloc[idx, self.SOURCE_IDX] == 0):\n",
    "\n",
    "                    img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                    if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "                        image = self.non_malignant_transform(image)\n",
    "                    elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "                        image = self.malignant_transform(image)\n",
    "\n",
    "                    return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0] \n",
    "                else:\n",
    "                    img_path = os.path.join(self.aug_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                    if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "                        image = self.non_malignant_transform(image)\n",
    "                    elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "                        image = self.malignant_transform(image)\n",
    "\n",
    "                    return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0] \n",
    "\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "                            std=[0.229, 0.224, 0.225])    # ImageNet stds\n",
    "    ])\n",
    "\n",
    "    malignant_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_dataset = ISICDataset(\n",
    "        csv_file='val_labels.csv',\n",
    "        img_dir=TEST_DIR[dir],\n",
    "        non_malignant_transform=transform,\n",
    "        malignant_transform=transform\n",
    "    )\n",
    "\n",
    "    train_dataset = ISICDataset(\n",
    "        csv_file='train_labels.csv',\n",
    "        img_dir=TRAIN_DIR[dir],\n",
    "        non_malignant_transform=malignant_transform,\n",
    "        malignant_transform=malignant_transform,\n",
    "        aug_dir = None, #AUG_DIR\n",
    "        train_bool = True,\n",
    "        Balance= False\n",
    "    )\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "    for model_name in MODELS:\n",
    "        resnet = initialize_model(model_name=model_name, num_classes=1)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=1e-4)\n",
    "        criterion = FocalLoss(alpha=0.9, gamma=2.0, reduction='mean').to(device)\n",
    "        #criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.0]).to(device))\n",
    "\n",
    "        # switch to this if we're doing more than malignant/not-malignant\n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        resnet.to(device)\n",
    "\n",
    "        with open(f\"training_log_{model_name}_{DATA_NAMES[dir]}.csv\", \"w\") as f:\n",
    "            f.write(\"epoch,train_loss,val_loss,val_tn,val_fn,val_tp,val_fp,val_accuracy,val_auc,pauc,f1score\\n\")\n",
    "            start_time = time.time()\n",
    "            for epoch in range(EPOCHS):\n",
    "                train_dataset.rebalance()\n",
    "                dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "                valloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "                print(f\"Training set size: {len(dataloader.dataset)}\")\n",
    "                print(f\"Validation set size: {len(valloader.dataset)}\")\n",
    "\n",
    "                resnet.train()\n",
    "                avgloss = 0.0\n",
    "                print(f\"EPOCH: {epoch + 1}\")\n",
    "                for images, labels, _ in dataloader:\n",
    "                    images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = resnet(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    avgloss += loss.item() * images.size(0)\n",
    "                avgloss /= len(dataloader.dataset)\n",
    "\n",
    "                resnet.eval()\n",
    "                val_loss = 0.0\n",
    "                total = 0\n",
    "                false_negative = 0\n",
    "                false_positive = 0\n",
    "                true_negative = 0\n",
    "                true_positive = 0\n",
    "                all_labels = []\n",
    "                all_probs = []  # Store probabilities instead of binary predictions\n",
    "                image_ids_list = []  # Collect image IDs for submission\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for images, labels, image_ids in valloader:  # Ensure dataset returns image IDs\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device).float().unsqueeze(1)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        outputs = resnet(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                        # Get probabilities (before thresholding)\n",
    "                        probabilities = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                        all_probs.extend(probabilities)\n",
    "                        \n",
    "                        # Get binary predictions for confusion matrix\n",
    "                        preds = (torch.sigmoid(outputs) > 0.8).float()\n",
    "                        preds = preds.cpu().numpy().flatten()\n",
    "                        \n",
    "                        # Track image IDs for submission\n",
    "                        image_ids_list.extend(image_ids)  # Assuming image_ids are strings\n",
    "                        \n",
    "                        # Update confusion matrix\n",
    "                        for p, l in zip(preds, labels.cpu().numpy()):\n",
    "                            if p == 0 and l == 0:\n",
    "                                true_negative += 1\n",
    "                            elif p == 0 and l == 1:\n",
    "                                false_negative += 1\n",
    "                            elif p == 1 and l == 0:\n",
    "                                false_positive += 1\n",
    "                            elif p == 1 and l == 1:\n",
    "                                true_positive += 1\n",
    "                        total += labels.size(0)\n",
    "                        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "                # Calculate metrics\n",
    "                avg_val_loss = val_loss / len(valloader.dataset)\n",
    "                accuracy = (true_positive + true_negative) / total\n",
    "                p_auc = roc_auc_score(all_labels, all_probs)  # Use probabilities for AUC\n",
    "                f1score = f1_score(all_labels, [1 if x > 0.5 else 0 for x in all_probs])\n",
    "\n",
    "                # Create submission DataFrame\n",
    "                submission_df = pd.DataFrame({\n",
    "                    'isic_id': image_ids_list,\n",
    "                    'prediction': all_probs\n",
    "                })\n",
    "\n",
    "                solution_df = pd.DataFrame({\n",
    "                    'isic_id': image_ids_list,\n",
    "                    'is_malignant': all_labels\n",
    "                })\n",
    "\n",
    "                # Calculate pAUC using the competition metric\n",
    "                try:\n",
    "                    pAUC = score(\n",
    "                        solution=solution_df,\n",
    "                        submission=submission_df,\n",
    "                        row_id_column_name='isic_id'  # Must match your column name\n",
    "                    )\n",
    "                except ParticipantVisibleError as e:\n",
    "                    print(f\"Scoring Error: {e}\")\n",
    "                    pAUC = -1  # Handle invalid submissions\n",
    "\n",
    "                if (epoch+1) % 5 == 0:\n",
    "                    torch.save(resnet.state_dict(), f\"resnet50_{epoch}_{DUPE}_{FREEZE}_pos_weight(2).pth\")\n",
    "                \n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "                    f\"Train Loss: {avgloss:.4f} \"\n",
    "                    f\"Val Loss: {avg_val_loss:.4f}\\n\"\n",
    "                    f\"Val TN: {true_negative} FN: {false_negative} \\n\"\n",
    "                    f\"TP: {true_positive} FP: {false_positive}\\n\"\n",
    "                    f\"Val Accuracy: {accuracy:.4f}\\n\"\n",
    "                    f\"AUC: {p_auc:.4f}\\n\"\n",
    "                    f\"pAUC: {pAUC:.4f}\\n\"\n",
    "                    f\"f1 score: {f1score:.4f}\\n\")  # Add pAUC to output'\n",
    "                f.write(f\"{epoch+1},{avgloss},{avg_val_loss},{true_negative},{false_negative},{true_positive},{false_positive},{accuracy},{p_auc},{pAUC},{f1score}\\n\")\n",
    "            \n",
    "            \n",
    "        print(f\"Training time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        with open(f\"scores_{model_name}_{DATA_NAMES[dir]}.csv\", \"w\") as f:\n",
    "            f.write(\"isic_id,prediction,target\\n\")\n",
    "            for id, fscore, actual in zip(image_ids_list, all_probs, all_labels):\n",
    "                f.write(f\"{id},{fscore},{actual}\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        def plot_roc_and_pauc(y_true, y_scores, min_tpr=0.80):\n",
    "            # Flip labels and predictions (based on your scoring function)\n",
    "            v_gt = abs(np.asarray(y_true) - 1)\n",
    "            v_pred = -1.0 * np.asarray(y_scores)\n",
    "\n",
    "            # Calculate ROC curve\n",
    "            fpr, tpr, thresholds = roc_curve(v_gt, v_pred)\n",
    "\n",
    "            # Compute full AUC and partial AUC\n",
    "            full_auc = auc(fpr, tpr)\n",
    "            max_fpr = abs(1 - min_tpr)\n",
    "\n",
    "            # Partial AUC calculation (manual interpolation for the cutoff point)\n",
    "            stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "            x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "            y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "            \n",
    "            # Interpolated TPR at max_fpr\n",
    "            interp_tpr = np.interp(max_fpr, x_interp, y_interp)\n",
    "            \n",
    "            # Create partial ROC arrays up to max_fpr\n",
    "            pauc_fpr = np.append(fpr[:stop], max_fpr)\n",
    "            pauc_tpr = np.append(tpr[:stop], interp_tpr)\n",
    "            \n",
    "            partial_auc = auc(pauc_fpr, pauc_tpr)\n",
    "\n",
    "            # Plotting\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {full_auc:.4f})\")\n",
    "            \n",
    "            # Shade pAUC region\n",
    "            plt.fill_between(pauc_fpr, pauc_tpr, step=\"post\", alpha=0.3, color=\"orange\", label=f\"pAUC = {partial_auc:.4f}\")\n",
    "            \n",
    "            # Draw horizontal line at TPR = min_tpr\n",
    "            plt.axhline(min_tpr, color='red', linestyle='--', label=f\"Min TPR = {min_tpr}\")\n",
    "\n",
    "            # Draw vertical line at FPR = max_fpr\n",
    "            plt.axvline(max_fpr, color='green', linestyle='--', label=f\"Max FPR = {max_fpr:.2f}\")\n",
    "\n",
    "            plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "            plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "            plt.title(\"ROC Curve with pAUC Region Highlighted\")\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"roc_curve_{model_name}_{DATA_NAMES[dir]}.png\")\n",
    "\n",
    "        # Example usage\n",
    "        plot_roc_and_pauc(all_labels, all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/forestz/.local/lib/python3.11/site-packages (from peft) (2.6.0)\n",
      "Collecting transformers (from peft)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from peft) (4.65.0)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in /home/forestz/.local/lib/python3.11/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /home/forestz/.local/lib/python3.11/site-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/forestz/.local/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/forestz/.local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from transformers->peft) (2023.10.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->peft)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.2.2)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers, accelerate, peft\n",
      "Successfully installed accelerate-1.6.0 peft-0.15.2 tokenizers-0.21.1 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48693\n"
     ]
    }
   ],
   "source": [
    "n = train_df[train_df.iloc[:, 0].isna()]\n",
    "\n",
    "print(len(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import torchvision\n",
    "import timm\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "# params\n",
    "FREEZE = True # whether or not to freeze the weights\n",
    "EPOCHS = 15 # number of training epochs\n",
    "DUPE = True\n",
    "DUPE_COUNT = 100\n",
    "#TRAIN_DIR = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train_images_hair_removed_dullrazor\\\\'\n",
    "TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train_dataset_with_skin_tone.csv'\n",
    "TRAIN_DIR = ['./ISIC_2024_Training_Input/']\n",
    "#TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train-metadata.csv'\n",
    "#TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\cleaned_styled_data\\\\cleaned_styled_data\\\\cleaned_augmented_data.csv'\n",
    "#TEST_DIR = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_images_hair_removed_dullrazor\\\\'\n",
    "#TEST_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_dataset_with_skin_tone.csv'\n",
    "\n",
    "MODELS = [\"efficientnet\", \"resnet50\", \"coatnet\", \"resnet34\", \"deit\"]\n",
    "\n",
    "DATA_NAMES = ['ISIC_2024_Training_Input']\n",
    "TEST_DIR = ['./ISIC_2024_Training_Input/', \"./ISIC_2024_Training_Input/\"]\n",
    "TEST_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_dataset_with_skin_tone.csv'\n",
    "\n",
    "\"\"\"\n",
    "2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def initialize_model(model_name, num_classes=1):\n",
    "    model = None\n",
    "    \n",
    "    if model_name == \"efficientnet\":\n",
    "        target_modules=[\n",
    "            # final linear\n",
    "            \"classifier.1\",\n",
    "            # a couple of the deepest MBConv depthwise convs:\n",
    "            \"features.6.block.2.conv\",  \n",
    "            \"features.7.block.2.conv\",  \n",
    "        ]\n",
    "\n",
    "        model = torchvision.models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        if FREEZE:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "    elif model_name == \"resnet34\":\n",
    "        target_modules=[\n",
    "            # final classification\n",
    "            \"fc\",\n",
    "            # pick one conv in each of the last 3 residual stages:\n",
    "            \"layer2.0.conv1\",\n",
    "            \"layer3.1.conv2\",\n",
    "            \"layer4.2.conv2\",\n",
    "        ]\n",
    "        model = torchvision.models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        if FREEZE:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    elif model_name == \"resnet50\":\n",
    "        target_modules=[\n",
    "            # final classification\n",
    "            \"fc\",\n",
    "            # pick one conv in each of the last 3 residual stages:\n",
    "            \"layer2.0.conv1\",\n",
    "            \"layer3.1.conv2\",\n",
    "            \"layer4.2.conv2\",\n",
    "        ]\n",
    "        model = torchvision.models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        if FREEZE:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    elif model_name == \"coatnet\":\n",
    "        target_modules=[\n",
    "            # final linear\n",
    "            \"head.fc\",\n",
    "            # point‑wise & depth‑wise conv in the last stage:\n",
    "            \"stages.3.blocks.2.pwconv\",\n",
    "            \"stages.3.blocks.2.conv\",  \n",
    "        ]\n",
    "\n",
    "        model = timm.create_model('coatnet_0_rw_224', pretrained=True)\n",
    "        if FREEZE:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.head.fc.in_features\n",
    "        model.head.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    elif model_name == \"deit\":\n",
    "        target_modules=[\n",
    "            # QKV + output projection in each of the first few heads:\n",
    "            \"blocks.0.attn.qkv\",   \"blocks.0.attn.proj\",\n",
    "            \"blocks.1.attn.qkv\",   \"blocks.1.attn.proj\",\n",
    "            # …repeat for as many blocks as you like…\n",
    "            # final classifier(s):\n",
    "            \"head\", \n",
    "            \"head_dist\",         # only for the distilled variant\n",
    "        ]\n",
    "\n",
    "        model = timm.create_model('deit_base_patch16_224', pretrained=True)\n",
    "        if FREEZE:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        num_ftrs = model.head.in_features\n",
    "        model.head = nn.Linear(num_ftrs, num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=1,\n",
    "        lora_alpha=2,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80) -> float:\n",
    "    '''\n",
    "    2024 ISIC Challenge metric: pAUC\n",
    "    \n",
    "    Given a solution file and submission file, this function returns the\n",
    "    the partial area under the receiver operating characteristic (pAUC) \n",
    "    above a given true positive rate (TPR) = 0.80.\n",
    "    https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "    \n",
    "    (c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\n",
    "    Args:\n",
    "        solution: ground truth pd.DataFrame of 1s and 0s\n",
    "        submission: solution dataframe of predictions of scores ranging [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        Float value range [0, max_fpr]\n",
    "    '''\n",
    "    for col in solution.columns:\n",
    "        if col != 'is_malignant':\n",
    "            del solution[col]\n",
    "    \n",
    "    for col in submission.columns:\n",
    "        if col != 'prediction':\n",
    "            del submission[col]\n",
    "\n",
    "    # check submission is numeric\n",
    "    if not pandas.api.types.is_numeric_dtype(submission.values):\n",
    "        raise ParticipantVisibleError('Submission target column must be numeric')\n",
    "\n",
    "    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)\n",
    "    v_gt = abs(solution.values.ravel()-1)\n",
    "    \n",
    "    # flip the submissions to their compliments\n",
    "    v_pred = -1.0*submission.values.ravel()\n",
    "\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "\n",
    "    # using sklearn.metric functions: (1) roc_curve and (2) auc\n",
    "    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n",
    "    if max_fpr is None or max_fpr == 1:\n",
    "        return auc(fpr, tpr)\n",
    "    if max_fpr <= 0 or max_fpr > 1:\n",
    "        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n",
    "        \n",
    "    # Add a single point at max_fpr by linear interpolation\n",
    "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "    fpr = np.append(fpr[:stop], max_fpr)\n",
    "    partial_auc = auc(fpr, tpr)\n",
    "\n",
    "#     # Equivalent code that uses sklearn's roc_auc_score\n",
    "#     v_gt = abs(np.asarray(solution.values)-1)\n",
    "#     v_pred = np.array([1.0 - x for x in submission.values])\n",
    "#     max_fpr = abs(1-min_tpr)\n",
    "#     partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "#     # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "#     # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "#     partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return(partial_auc)\n",
    "\n",
    "for dir in range(len(TRAIN_DIR)):\n",
    "    AUG = False\n",
    "    USE_SPLIT = False\n",
    "\n",
    "    MALIGNANT = 'target'\n",
    "    # MALIGNANT = 'target'\n",
    "    MALIG_IDX = 1\n",
    "    # MALIG_IDX = 3\n",
    "\n",
    "    if not TEST_DIR:\n",
    "        TEST_DIR = TRAIN_DIR[dir]\n",
    "        USE_SPLIT = True\n",
    "    if not TEST_CSV:\n",
    "        TEST_CSV = TRAIN_CSV\n",
    "        USE_SPLIT = True\n",
    "\n",
    "    def balance_classes(df, label_col=MALIGNANT):\n",
    "         # Separate malignant and non-malignant samples\n",
    "         malig_df = df[df[label_col] == 1]\n",
    "         non_malig_df = df[df[label_col] == 0]\n",
    "\n",
    "         # Get the minority count\n",
    "         min_count = min(len(malig_df), len(non_malig_df))\n",
    "\n",
    "         # Downsample both classes to min_count (or oversample malignant if needed)\n",
    "         non_malig_df_balanced = non_malig_df.sample(n=min_count, random_state=42)\n",
    "         malig_df_balanced = malig_df.sample(n=min_count, replace=True, random_state=42)  # oversample if needed\n",
    "\n",
    "         # Concatenate back\n",
    "         balanced_df = pd.concat([malig_df_balanced, non_malig_df_balanced]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "         return balanced_df\n",
    "\n",
    "    if USE_SPLIT:\n",
    "        df = pd.read_csv(TRAIN_CSV)\n",
    "        #df = pd.read_csv('C:\\\\Users\\\\rngki\\\\Downloads\\\\cleaned_styled_data\\\\cleaned_styled_data\\\\cleaned_augmented_data.csv')\n",
    "        print(f\"num malignant: {sum(df[MALIGNANT])}\")\n",
    "\n",
    "        # downsample for time\n",
    "        df = df.sample(frac=.1, random_state=42)\n",
    "\n",
    "        train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[MALIGNANT], random_state=42)\n",
    "    elif AUG:\n",
    "        OG_train_df = pd.read_csv(TRAIN_CSV)\n",
    "        AUG_train = pd.read_csv(AUG_CSV)\n",
    "        # Add a source column to each\n",
    "        OG_train_df[\"source\"] = 0\n",
    "        AUG_train[\"source\"] = 1\n",
    "\n",
    "        #SOURCE_IDX = AUG_train.columns.get_loc(\"source\")\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "        train_df = pd.concat([OG_train_df, AUG_train], ignore_index=True)\n",
    "        val_df = pd.read_csv(TEST_CSV)\n",
    "    else:\n",
    "        train_df = pd.read_csv(TRAIN_CSV)\n",
    "        \n",
    "        val_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "    if DUPE:\n",
    "        malig_df = train_df[train_df[MALIGNANT] == 1]\n",
    "        malig_df = pd.concat([malig_df]*DUPE_COUNT, ignore_index=True)\n",
    "\n",
    "        train_df = pd.concat([train_df, malig_df], ignore_index=True)\n",
    "\n",
    "    train_df = balance_classes(train_df)\n",
    "   # train_df = train_df.sample(frac=.001, random_state=42)\n",
    "    #val_df = val_df.sample(frac=.1, random_state=42)\n",
    "\n",
    "    train_df.to_csv('train_labels.csv', index=False)\n",
    "    val_df.to_csv('val_labels.csv', index=False)\n",
    "\n",
    "    print(f\"Validation - Num malignant: {val_df[MALIGNANT].sum()}\")\n",
    "    print(f\"Validation - Num benign: {len(val_df) - val_df[MALIGNANT].sum()}\")\n",
    "    print(f\"Training - Num malignant: {train_df[MALIGNANT].sum()}\")\n",
    "    print(f\"Training - Num benign: {len(train_df) - train_df[MALIGNANT].sum()}\")\n",
    "\n",
    "    class ISICDataset(Dataset):\n",
    "        def __init__(self, csv_file, img_dir, non_malignant_transform=None, malignant_transform=None,label_col=MALIGNANT,aug_dir = None, train_bool = False):\n",
    "            self.fulldata = pd.read_csv(csv_file)\n",
    "            self.train = train_bool\n",
    "            if self.train:\n",
    "                self.malig_df = self.fulldata[self.fulldata[label_col] ==1]\n",
    "                self.non_malig_df = self.fulldata[self.fulldata[label_col] ==0]\n",
    "                self.min_count = min(len(self.malig_df), len(self.non_malig_df))\n",
    "                non_malig_df_balanced = self.non_malig_df.sample(n=self.min_count)\n",
    "\n",
    "                # Concatenate back\n",
    "                self.df = pd.concat([self.malig_df, non_malig_df_balanced]).sample(frac=1).reset_index(drop=True)\n",
    "                self.SOURCE_IDX = self.df.columns.get_loc(\"source\")\n",
    "            else:\n",
    "                self.df = self.fulldata\n",
    "            \n",
    "            self.img_dir = img_dir\n",
    "            self.aug_dir = aug_dir\n",
    "            self.non_malignant_transform = non_malignant_transform\n",
    "            self.malignant_transform = malignant_transform\n",
    "\n",
    "        def rebalance(self):\n",
    "            if not self.train:\n",
    "                return\n",
    "\n",
    "            non_malig_df_balanced = self.non_malig_df.sample(n=self.min_count)\n",
    "            # Concatenate back\n",
    "            self.df = pd.concat([self.malig_df, non_malig_df_balanced]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if(self.aug_dir == None):\n",
    "                img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "                    image = self.non_malignant_transform(image)\n",
    "                elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "                    image = self.malignant_transform(image)\n",
    "\n",
    "                return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0]\n",
    "            else:\n",
    "                if(self.df.iloc[idx, self.SOURCE_IDX] == 0):\n",
    "\n",
    "                    img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                    if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "                        image = self.non_malignant_transform(image)\n",
    "                    elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "                        image = self.malignant_transform(image)\n",
    "\n",
    "                    return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0] \n",
    "                else:\n",
    "\n",
    "                    img_path = os.path.join(self.aug_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                    if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "                        image = self.non_malignant_transform(image)\n",
    "                    elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "                        image = self.malignant_transform(image)\n",
    "\n",
    "                    return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0] \n",
    "\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "                            std=[0.229, 0.224, 0.225])    # ImageNet stds\n",
    "    ])\n",
    "\n",
    "    malignant_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_dataset = ISICDataset(\n",
    "        csv_file='val_labels.csv',\n",
    "        img_dir=TEST_DIR[dir],\n",
    "        non_malignant_transform=transform,\n",
    "        malignant_transform=transform\n",
    "    )\n",
    "\n",
    "    train_dataset = ISICDataset(\n",
    "        csv_file='train_labels.csv',\n",
    "        img_dir=TRAIN_DIR[dir],\n",
    "        non_malignant_transform=malignant_transform,\n",
    "        malignant_transform=malignant_transform\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    valloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    print(f\"Training set size: {len(dataloader.dataset)}\")\n",
    "    print(f\"Validation set size: {len(valloader.dataset)}\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "    for model_name in MODELS:\n",
    "        resnet = initialize_model(model_name=model_name, num_classes=1)\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=1e-4)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))\n",
    "\n",
    "        # switch to this if we're doing more than malignant/not-malignant\n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        resnet.to(device)\n",
    "\n",
    "        with open(f\"training_log_{model_name}_{DATA_NAMES[dir]}.csv\", \"w\") as f:\n",
    "            f.write(\"epoch,train_loss,val_loss,val_tn,val_fn,val_tp,val_fp,val_accuracy,val_auc,pauc,f1score\\n\")\n",
    "            start_time = time.time()\n",
    "            for epoch in range(EPOCHS):\n",
    "                train_dataset.rebalance()\n",
    "                dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "                valloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "                resnet.train()\n",
    "                avgloss = 0.0\n",
    "                print(f\"EPOCH: {epoch + 1}\")\n",
    "                for images, labels, _ in dataloader:\n",
    "                    images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = resnet(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    avgloss += loss.item() * images.size(0)\n",
    "                avgloss /= len(dataloader.dataset)\n",
    "\n",
    "                resnet.eval()\n",
    "                val_loss = 0.0\n",
    "                total = 0\n",
    "                false_negative = 0\n",
    "                false_positive = 0\n",
    "                true_negative = 0\n",
    "                true_positive = 0\n",
    "                all_labels = []\n",
    "                all_probs = []  # Store probabilities instead of binary predictions\n",
    "                image_ids_list = []  # Collect image IDs for submission\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for images, labels, image_ids in valloader:  # Ensure dataset returns image IDs\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device).float().unsqueeze(1)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        outputs = resnet(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                        # Get probabilities (before thresholding)\n",
    "                        probabilities = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                        all_probs.extend(probabilities)\n",
    "                        \n",
    "                        # Get binary predictions for confusion matrix\n",
    "                        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                        preds = preds.cpu().numpy().flatten()\n",
    "                        \n",
    "                        # Track image IDs for submission\n",
    "                        image_ids_list.extend(image_ids)  # Assuming image_ids are strings\n",
    "                        \n",
    "                        # Update confusion matrix\n",
    "                        for p, l in zip(preds, labels.cpu().numpy()):\n",
    "                            if p == 0 and l == 0:\n",
    "                                true_negative += 1\n",
    "                            elif p == 0 and l == 1:\n",
    "                                false_negative += 1\n",
    "                            elif p == 1 and l == 0:\n",
    "                                false_positive += 1\n",
    "                            elif p == 1 and l == 1:\n",
    "                                true_positive += 1\n",
    "                        total += labels.size(0)\n",
    "                        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "                # Calculate metrics\n",
    "                avg_val_loss = val_loss / len(valloader.dataset)\n",
    "                accuracy = (true_positive + true_negative) / total\n",
    "                p_auc = roc_auc_score(all_labels, all_probs)  # Use probabilities for AUC\n",
    "                f1score = f1_score(all_labels, [1 if x > 0.5 else 0 for x in all_probs])\n",
    "\n",
    "                # Create submission DataFrame\n",
    "                submission_df = pd.DataFrame({\n",
    "                    'isic_id': image_ids_list,\n",
    "                    'prediction': all_probs\n",
    "                })\n",
    "\n",
    "                solution_df = pd.DataFrame({\n",
    "                    'isic_id': image_ids_list,\n",
    "                    'is_malignant': all_labels\n",
    "                })\n",
    "\n",
    "                # Calculate pAUC using the competition metric\n",
    "                try:\n",
    "                    pAUC = score(\n",
    "                        solution=solution_df,\n",
    "                        submission=submission_df,\n",
    "                        row_id_column_name='isic_id'  # Must match your column name\n",
    "                    )\n",
    "                except ParticipantVisibleError as e:\n",
    "                    print(f\"Scoring Error: {e}\")\n",
    "                    pAUC = -1  # Handle invalid submissions\n",
    "\n",
    "                if (epoch+1) % 5 == 0:\n",
    "                    torch.save(resnet.state_dict(), f\"resnet50_{epoch}_{DUPE}_{FREEZE}_pos_weight(2).pth\")\n",
    "                \n",
    "                print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "                    f\"Train Loss: {avgloss:.4f} \"\n",
    "                    f\"Val Loss: {avg_val_loss:.4f}\\n\"\n",
    "                    f\"Val TN: {true_negative} FN: {false_negative} \\n\"\n",
    "                    f\"TP: {true_positive} FP: {false_positive}\\n\"\n",
    "                    f\"Val Accuracy: {accuracy:.4f}\\n\"\n",
    "                    f\"AUC: {p_auc:.4f}\\n\"\n",
    "                    f\"pAUC: {pAUC:.4f}\\n\"\n",
    "                    f\"f1 score: {f1score:.4f}\\n\")  # Add pAUC to output'\n",
    "                f.write(f\"{epoch+1},{avgloss},{avg_val_loss},{true_negative},{false_negative},{true_positive},{false_positive},{accuracy},{p_auc},{pAUC},{f1score}\\n\")\n",
    "            \n",
    "            \n",
    "        print(f\"Training time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        with open(f\"scores_{model_name}_{DATA_NAMES[dir]}.csv\", \"w\") as f:\n",
    "            f.write(\"isic_id,prediction,target\\n\")\n",
    "            for id, fscore, actual in zip(image_ids_list, all_probs, all_labels):\n",
    "                f.write(f\"{id},{fscore},{actual}\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        def plot_roc_and_pauc(y_true, y_scores, min_tpr=0.80):\n",
    "            # Flip labels and predictions (based on your scoring function)\n",
    "            v_gt = abs(np.asarray(y_true) - 1)\n",
    "            v_pred = -1.0 * np.asarray(y_scores)\n",
    "\n",
    "            # Calculate ROC curve\n",
    "            fpr, tpr, thresholds = roc_curve(v_gt, v_pred)\n",
    "\n",
    "            # Compute full AUC and partial AUC\n",
    "            full_auc = auc(fpr, tpr)\n",
    "            max_fpr = abs(1 - min_tpr)\n",
    "\n",
    "            # Partial AUC calculation (manual interpolation for the cutoff point)\n",
    "            stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "            x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "            y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "            \n",
    "            # Interpolated TPR at max_fpr\n",
    "            interp_tpr = np.interp(max_fpr, x_interp, y_interp)\n",
    "            \n",
    "            # Create partial ROC arrays up to max_fpr\n",
    "            pauc_fpr = np.append(fpr[:stop], max_fpr)\n",
    "            pauc_tpr = np.append(tpr[:stop], interp_tpr)\n",
    "            \n",
    "            partial_auc = auc(pauc_fpr, pauc_tpr)\n",
    "\n",
    "            # Plotting\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {full_auc:.4f})\")\n",
    "            \n",
    "            # Shade pAUC region\n",
    "            plt.fill_between(pauc_fpr, pauc_tpr, step=\"post\", alpha=0.3, color=\"orange\", label=f\"pAUC = {partial_auc:.4f}\")\n",
    "            \n",
    "            # Draw horizontal line at TPR = min_tpr\n",
    "            plt.axhline(min_tpr, color='red', linestyle='--', label=f\"Min TPR = {min_tpr}\")\n",
    "\n",
    "            # Draw vertical line at FPR = max_fpr\n",
    "            plt.axvline(max_fpr, color='green', linestyle='--', label=f\"Max FPR = {max_fpr:.2f}\")\n",
    "\n",
    "            plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "            plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "            plt.title(\"ROC Curve with pAUC Region Highlighted\")\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"roc_curve_{model_name}_{DATA_NAMES[dir]}.png\")\n",
    "\n",
    "        # Example usage\n",
    "        plot_roc_and_pauc(all_labels, all_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
