{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the ISIC dataset stuff\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2024/ISIC_2024_Training_Input.zip\n",
    "!unzip ISIC_2024_Training_Input.zip\n",
    "\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2024/ISIC_2024_Training_Supplement.csv\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2024/ISIC_2024_Training_GroundTruth.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "FREEZE = True # whether or not to freeze the weights\n",
    "EPOCHS = 10 # number of training epochs\n",
    "DUPE = True\n",
    "#TRAIN_DIR = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train_images_hair_removed_dullrazor\\\\'\n",
    "TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train_dataset_with_skin_tone.csv'\n",
    "TRAIN_DIR = './ISIC_2024_Training_Input/'\n",
    "#TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\train-metadata.csv'\n",
    "#TRAIN_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\cleaned_styled_data\\\\cleaned_styled_data\\\\cleaned_augmented_data.csv'\n",
    "#TEST_DIR = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_images_hair_removed_dullrazor\\\\'\n",
    "#TEST_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_dataset_with_skin_tone.csv'\n",
    "TEST_DIR = './ISIC_2024_Training_Input/'\n",
    "TEST_CSV = 'C:\\\\Users\\\\rngki\\\\Downloads\\\\test_dataset_with_skin_tone.csv'\n",
    "#TEST_DIR = ''\n",
    "#TEST_CSV = '' \n",
    "\n",
    "USE_SPLIT = False\n",
    "\n",
    "MALIGNANT = 'target'\n",
    "# MALIGNANT = 'target'\n",
    "MALIG_IDX = 1\n",
    "# MALIG_IDX = 3\n",
    "\n",
    "if not TEST_DIR:\n",
    "    TEST_DIR = TRAIN_DIR\n",
    "    USE_SPLIT = True\n",
    "if not TEST_CSV:\n",
    "    TEST_CSV = TRAIN_CSV\n",
    "    USE_SPLIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rngki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# setup the model to be transfer learned\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rngki\\AppData\\Local\\Temp\\ipykernel_68428\\888517920.py:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(TRAIN_CSV)\n",
      "C:\\Users\\rngki\\AppData\\Local\\Temp\\ipykernel_68428\\888517920.py:12: DtypeWarning: Columns (52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_df = pd.read_csv(TEST_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Num malignant: 79\n",
      "Validation - Num benign: 80133\n",
      "Training - Num malignant: 31714\n",
      "Training - Num benign: 31714\n",
      "Training set size: 63428\n",
      "Validation set size: 80212\n",
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rngki\\AppData\\Local\\Temp\\ipykernel_68428\\888517920.py:55: DtypeWarning: Columns (52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.df = pd.read_csv(csv_file)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_SPLIT:\n",
    "    df = pd.read_csv(TRAIN_CSV)\n",
    "    #df = pd.read_csv('C:\\\\Users\\\\rngki\\\\Downloads\\\\cleaned_styled_data\\\\cleaned_styled_data\\\\cleaned_augmented_data.csv')\n",
    "    print(f\"num malignant: {sum(df[MALIGNANT])}\")\n",
    "\n",
    "    # downsample for time\n",
    "    df = df.sample(frac=.1, random_state=42)\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[MALIGNANT], random_state=42)\n",
    "else:\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    val_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "if DUPE:\n",
    "    malig_df = train_df[train_df[MALIGNANT] == 1]\n",
    "    malig_df = pd.concat([malig_df]*100, ignore_index=True)\n",
    "\n",
    "    train_df = pd.concat([train_df, malig_df], ignore_index=True)\n",
    "\n",
    "    #malig_df = val_df[val_df['malignant'] == 1]\n",
    "    #malig_df = pd.concat([malig_df]*100, ignore_index=True)\n",
    "\n",
    "    #val_df = pd.concat([val_df, malig_df], ignore_index=True)\n",
    "\n",
    "\n",
    "def balance_classes(df, label_col=MALIGNANT):\n",
    "    # Separate malignant and non-malignant samples\n",
    "    malig_df = df[df[label_col] == 1]\n",
    "    non_malig_df = df[df[label_col] == 0]\n",
    "\n",
    "    # Get the minority count\n",
    "    min_count = min(len(malig_df), len(non_malig_df))\n",
    "\n",
    "    # Downsample both classes to min_count (or oversample malignant if needed)\n",
    "    non_malig_df_balanced = non_malig_df.sample(n=min_count, random_state=42)\n",
    "    malig_df_balanced = malig_df.sample(n=min_count, replace=True, random_state=42)  # oversample if needed\n",
    "\n",
    "    # Concatenate back\n",
    "    balanced_df = pd.concat([malig_df_balanced, non_malig_df_balanced]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return balanced_df\n",
    "\n",
    "# Balance the training set\n",
    "train_df = balance_classes(train_df)\n",
    "\n",
    "train_df.to_csv('train_labels.csv', index=False)\n",
    "val_df.to_csv('val_labels.csv', index=False)\n",
    "\n",
    "print(f\"Validation - Num malignant: {val_df[MALIGNANT].sum()}\")\n",
    "print(f\"Validation - Num benign: {len(val_df) - val_df[MALIGNANT].sum()}\")\n",
    "print(f\"Training - Num malignant: {train_df[MALIGNANT].sum()}\")\n",
    "print(f\"Training - Num benign: {len(train_df) - train_df[MALIGNANT].sum()}\")\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, non_malignant_transform=None, malignant_transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.non_malignant_transform = non_malignant_transform\n",
    "        self.malignant_transform = malignant_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.df.iloc[idx, 0] + '.jpg')\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.df.iloc[idx, MALIG_IDX] == 0 and self.non_malignant_transform:\n",
    "            image = self.non_malignant_transform(image)\n",
    "        elif self.df.iloc[idx, MALIG_IDX] == 1 and self.malignant_transform:\n",
    "            image = self.malignant_transform(image)\n",
    "\n",
    "        return image, int(self.df.iloc[idx, MALIG_IDX]), self.df.iloc[idx, 0]\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "                         std=[0.229, 0.224, 0.225])    # ImageNet stds\n",
    "])\n",
    "\n",
    "malignant_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=30),\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    #transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_dataset = ISICDataset(\n",
    "    csv_file='val_labels.csv',\n",
    "    img_dir=TEST_DIR,\n",
    "    non_malignant_transform=transform,\n",
    "    malignant_transform=transform\n",
    ")\n",
    "\n",
    "train_dataset = ISICDataset(\n",
    "    csv_file='train_labels.csv',\n",
    "    img_dir=TRAIN_DIR,\n",
    "    non_malignant_transform=transform,\n",
    "    malignant_transform=malignant_transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Training set size: {len(dataloader.dataset)}\")\n",
    "print(f\"Validation set size: {len(valloader.dataset)}\")\n",
    "#malig = sum([x[1] for x in dataloader.dataset])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LORA with dropout\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "\n",
    "if FREEZE:\n",
    "  for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# set to 1 for now (either malignant or not, can change based on metadata later)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 1)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank adaptation size\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[ \"fc\",             # Final classification layer\n",
    "        \"layer2.0.conv1\", # First conv in layer2\n",
    "        \"layer3.1.conv2\",]  # Apply LoRA to the fully connected layer\n",
    ")\n",
    "\n",
    "resnet = get_peft_model(resnet, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROPOUT only\n",
    "class ResNetWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # Add spatial dropout between blocks\n",
    "        self.backbone.layer2.add_module(\"dropout\", nn.Dropout2d(dropout_prob))\n",
    "        self.backbone.layer3.add_module(\"dropout\", nn.Dropout2d(dropout_prob))\n",
    "        \n",
    "        # Final layer dropout\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(self.backbone.fc.in_features, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "resnet = ResNetWithDropout(dropout_prob=0.15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BalancedDisparityLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None):\n",
    "        \"\"\"\n",
    "        gamma: Focal loss parameter to focus more on hard examples\n",
    "        alpha: Optional class weighting for imbalanced data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # Will be computed dynamically if not provided\n",
    "\n",
    "    def forward(self, outputs, targets, skin_tones):\n",
    "        \"\"\"\n",
    "        outputs: Model logits\n",
    "        targets: Ground truth labels (0 or 1)\n",
    "        skin_tones: Tensor indicating skin tone groups for fairness evaluation\n",
    "        \"\"\"\n",
    "        pred_probs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # Compute dynamic class weights if alpha is not provided\n",
    "        if self.alpha is None:\n",
    "            pos_weight = (targets == 0).sum() / (targets == 1).sum()\n",
    "        else:\n",
    "            pos_weight = self.alpha\n",
    "\n",
    "        # ---- Focal Loss with Class Balance ----\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(outputs, targets.float(), reduction=\"none\", pos_weight=pos_weight)\n",
    "        pt = torch.exp(-bce_loss)  # Probability of correct classification\n",
    "        focal_loss = (1 - pt) ** self.gamma * bce_loss  # Focal weighting\n",
    "        base_loss = focal_loss.mean()\n",
    "\n",
    "        # ---- Performance Disparity Calculation ----\n",
    "        fprs, fnrs = [], []\n",
    "        for tone in torch.unique(skin_tones):\n",
    "            mask = (skin_tones == tone)\n",
    "            if mask.sum() > 0:\n",
    "                tn, fp, fn, tp = self._confusion_matrix(pred_probs[mask], targets[mask])\n",
    "                fprs.append(fp / (fp + tn + 1e-6))  \n",
    "                fnrs.append(fn / (fn + tp + 1e-6))\n",
    "\n",
    "        # Use Wasserstein Distance for better disparity measurement\n",
    "        fpr_disparity = torch.std(torch.stack(fprs))  # Standard deviation as disparity\n",
    "        fnr_disparity = torch.std(torch.stack(fnrs))\n",
    "\n",
    "        # ---- Final Weighted Loss ----\n",
    "        return base_loss + 0.4 * fpr_disparity + 0.6 * fnr_disparity  \n",
    "\n",
    "    def _confusion_matrix(self, probs, targets):\n",
    "        preds = (probs > 0.5).float()\n",
    "        tp = (preds * targets).sum()\n",
    "        fp = (preds * (1 - targets)).sum()\n",
    "        tn = ((1 - preds) * (1 - targets)).sum()\n",
    "        fn = ((1 - preds) * targets).sum()\n",
    "        return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2024 ISIC Challenge primary prize scoring metric\n",
    "\n",
    "Given a list of binary labels, an associated list of prediction \n",
    "scores ranging from [0,1], this function produces, as a single value, \n",
    "the partial area under the receiver operating characteristic (pAUC) \n",
    "above a given true positive rate (TPR).\n",
    "https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "\n",
    "(c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80) -> float:\n",
    "    '''\n",
    "    2024 ISIC Challenge metric: pAUC\n",
    "    \n",
    "    Given a solution file and submission file, this function returns the\n",
    "    the partial area under the receiver operating characteristic (pAUC) \n",
    "    above a given true positive rate (TPR) = 0.80.\n",
    "    https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
    "    \n",
    "    (c) 2024 Nicholas R Kurtansky, MSKCC\n",
    "\n",
    "    Args:\n",
    "        solution: ground truth pd.DataFrame of 1s and 0s\n",
    "        submission: solution dataframe of predictions of scores ranging [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        Float value range [0, max_fpr]\n",
    "    '''\n",
    "    for col in solution.columns:\n",
    "        if col != 'is_malignant':\n",
    "            del solution[col]\n",
    "    \n",
    "    for col in submission.columns:\n",
    "        if col != 'prediction':\n",
    "            del submission[col]\n",
    "\n",
    "    # check submission is numeric\n",
    "    if not pandas.api.types.is_numeric_dtype(submission.values):\n",
    "        raise ParticipantVisibleError('Submission target column must be numeric')\n",
    "\n",
    "    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)\n",
    "    v_gt = abs(solution.values.ravel()-1)\n",
    "    \n",
    "    # flip the submissions to their compliments\n",
    "    v_pred = -1.0*submission.values.ravel()\n",
    "\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "\n",
    "    # using sklearn.metric functions: (1) roc_curve and (2) auc\n",
    "    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n",
    "    if max_fpr is None or max_fpr == 1:\n",
    "        return auc(fpr, tpr)\n",
    "    if max_fpr <= 0 or max_fpr > 1:\n",
    "        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n",
    "        \n",
    "    # Add a single point at max_fpr by linear interpolation\n",
    "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "    fpr = np.append(fpr[:stop], max_fpr)\n",
    "    partial_auc = auc(fpr, tpr)\n",
    "\n",
    "#     # Equivalent code that uses sklearn's roc_auc_score\n",
    "#     v_gt = abs(np.asarray(solution.values)-1)\n",
    "#     v_pred = np.array([1.0 - x for x in submission.values])\n",
    "#     max_fpr = abs(1-min_tpr)\n",
    "#     partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "#     # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "#     # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "#     partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return(partial_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "Epoch [1/10] Train Loss: 0.0686 Val Loss: 0.0622\n",
      "Val TN: 78583 FN: 41 \n",
      "TP: 38 FP: 1550\n",
      "Val Accuracy: 0.9802\n",
      "AUC: 0.8928\n",
      "pAUC: 0.1141\n",
      "f1 score: 0.0456\n",
      "\n",
      "EPOCH: 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=1e-4)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([(len(dataloader.dataset) - malig)/malig]).to(device))\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))\n",
    "\n",
    "# switch to this if we're doing more than malignant/not-malignant\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "resnet.to(device)\n",
    "\n",
    "with open(\"training_log.csv\", \"w\") as f:\n",
    "    f.write(\"epoch,train_loss,val_loss,val_tn,val_fn,val_tp,val_fp,val_accuracy,val_auc,pauc,f1score\\n\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        resnet.train()\n",
    "        avgloss = 0.0\n",
    "        print(f\"EPOCH: {epoch + 1}\")\n",
    "        for images, labels, _ in dataloader:\n",
    "            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = resnet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avgloss += loss.item() * images.size(0)\n",
    "        avgloss /= len(dataloader.dataset)\n",
    "\n",
    "        resnet.eval()\n",
    "        val_loss = 0.0\n",
    "        total = 0\n",
    "        false_negative = 0\n",
    "        false_positive = 0\n",
    "        true_negative = 0\n",
    "        true_positive = 0\n",
    "        all_labels = []\n",
    "        all_probs = []  # Store probabilities instead of binary predictions\n",
    "        image_ids_list = []  # Collect image IDs for submission\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, image_ids in valloader:  # Ensure dataset returns image IDs\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = resnet(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                # Get probabilities (before thresholding)\n",
    "                probabilities = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                all_probs.extend(probabilities)\n",
    "                \n",
    "                # Get binary predictions for confusion matrix\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                preds = preds.cpu().numpy().flatten()\n",
    "                \n",
    "                # Track image IDs for submission\n",
    "                image_ids_list.extend(image_ids)  # Assuming image_ids are strings\n",
    "                \n",
    "                # Update confusion matrix\n",
    "                for p, l in zip(preds, labels.cpu().numpy()):\n",
    "                    if p == 0 and l == 0:\n",
    "                        true_negative += 1\n",
    "                    elif p == 0 and l == 1:\n",
    "                        false_negative += 1\n",
    "                    elif p == 1 and l == 0:\n",
    "                        false_positive += 1\n",
    "                    elif p == 1 and l == 1:\n",
    "                        true_positive += 1\n",
    "                total += labels.size(0)\n",
    "                all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_val_loss = val_loss / len(valloader.dataset)\n",
    "        accuracy = (true_positive + true_negative) / total\n",
    "        p_auc = roc_auc_score(all_labels, all_probs)  # Use probabilities for AUC\n",
    "        f1score = f1_score(all_labels, [1 if x > 0.5 else 0 for x in all_probs])\n",
    "\n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame({\n",
    "            'isic_id': image_ids_list,\n",
    "            'prediction': all_probs\n",
    "        })\n",
    "\n",
    "        solution_df = pd.DataFrame({\n",
    "            'isic_id': image_ids_list,\n",
    "            'is_malignant': all_labels\n",
    "        })\n",
    "\n",
    "        # Calculate pAUC using the competition metric\n",
    "        try:\n",
    "            pAUC = score(\n",
    "                solution=solution_df,\n",
    "                submission=submission_df,\n",
    "                row_id_column_name='isic_id'  # Must match your column name\n",
    "            )\n",
    "        except ParticipantVisibleError as e:\n",
    "            print(f\"Scoring Error: {e}\")\n",
    "            pAUC = -1  # Handle invalid submissions\n",
    "\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(resnet.state_dict(), f\"resnet50_{epoch}_{DUPE}_{FREEZE}_pos_weight(2).pth\")\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "            f\"Train Loss: {avgloss:.4f} \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}\\n\"\n",
    "            f\"Val TN: {true_negative} FN: {false_negative} \\n\"\n",
    "            f\"TP: {true_positive} FP: {false_positive}\\n\"\n",
    "            f\"Val Accuracy: {accuracy:.4f}\\n\"\n",
    "            f\"AUC: {p_auc:.4f}\\n\"\n",
    "            f\"pAUC: {pAUC:.4f}\\n\"\n",
    "            f\"f1 score: {f1score:.4f}\\n\")  # Add pAUC to output'\n",
    "        f.write(f\"{epoch+1},{avgloss},{avg_val_loss},{true_negative},{false_negative},{true_positive},{false_positive},{accuracy},{p_auc},{pAUC},{f1score}\\n\")\n",
    "    \n",
    "    \n",
    "print(f\"Training time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), 'resnet50_dropout_lora.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scores.csv\", \"w\") as f:\n",
    "    f.write(\"isic_id,prediction,target\\n\")\n",
    "    for id, score, actual in zip(image_ids_list, all_probs, all_labels):\n",
    "        f.write(f\"{id},{score},{actual}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_and_pauc(y_true, y_scores, min_tpr=0.80):\n",
    "    # Flip labels and predictions (based on your scoring function)\n",
    "    v_gt = abs(np.asarray(y_true) - 1)\n",
    "    v_pred = -1.0 * np.asarray(y_scores)\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(v_gt, v_pred)\n",
    "\n",
    "    # Compute full AUC and partial AUC\n",
    "    full_auc = auc(fpr, tpr)\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "\n",
    "    # Partial AUC calculation (manual interpolation for the cutoff point)\n",
    "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "    \n",
    "    # Interpolated TPR at max_fpr\n",
    "    interp_tpr = np.interp(max_fpr, x_interp, y_interp)\n",
    "    \n",
    "    # Create partial ROC arrays up to max_fpr\n",
    "    pauc_fpr = np.append(fpr[:stop], max_fpr)\n",
    "    pauc_tpr = np.append(tpr[:stop], interp_tpr)\n",
    "    \n",
    "    partial_auc = auc(pauc_fpr, pauc_tpr)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {full_auc:.4f})\")\n",
    "    \n",
    "    # Shade pAUC region\n",
    "    plt.fill_between(pauc_fpr, pauc_tpr, step=\"post\", alpha=0.3, color=\"orange\", label=f\"pAUC = {partial_auc:.4f}\")\n",
    "    \n",
    "    # Draw horizontal line at TPR = min_tpr\n",
    "    plt.axhline(min_tpr, color='red', linestyle='--', label=f\"Min TPR = {min_tpr}\")\n",
    "\n",
    "    # Draw vertical line at FPR = max_fpr\n",
    "    plt.axvline(max_fpr, color='green', linestyle='--', label=f\"Max FPR = {max_fpr:.2f}\")\n",
    "\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "    plt.title(\"ROC Curve with pAUC Region Highlighted\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_roc_and_pauc(all_labels, all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the inverse normalization transformation\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "    std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    ")\n",
    "\n",
    "def display_images(images, labels, preds, names, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    for i, (img, label, pred, name) in enumerate(zip(images, labels, preds, names)):\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        img = inv_normalize(img)  # Undo normalization\n",
    "        ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "        ax.set_title(f\"Name: {name}\\nLabel: {label.item()}, Pred: {pred.item()}\")\n",
    "        ax.axis('off')\n",
    "    # Hide any remaining subplots if there are fewer than 10 images\n",
    "    for j in range(i + 1, 10):\n",
    "        axes[j // 5, j % 5].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_images_by_type(dataloader, model, device, type, num_images=10):\n",
    "    images, labels, preds, names = [], [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for img_batch, label_batch, name_batch in dataloader:\n",
    "            img_batch, label_batch = img_batch.to(device), label_batch.to(device).float().unsqueeze(1)\n",
    "            output_batch = model(img_batch)\n",
    "            pred_batch = torch.sigmoid(output_batch)\n",
    "            pred_batch = (pred_batch > 0.5).float()\n",
    "            for img, label, pred, name in zip(img_batch, label_batch, pred_batch, name_batch):\n",
    "                if type == 'true_negative' and label == 0 and pred == 0:\n",
    "                    images.append(img)\n",
    "                    labels.append(label)\n",
    "                    preds.append(pred)\n",
    "                    names.append(name)\n",
    "                elif type == 'true_positive' and label == 1 and pred == 1:\n",
    "                    images.append(img)\n",
    "                    labels.append(label)\n",
    "                    preds.append(pred)\n",
    "                    names.append(name)\n",
    "                elif type == 'false_negative' and label == 1 and pred == 0:\n",
    "                    images.append(img)\n",
    "                    labels.append(label)\n",
    "                    preds.append(pred)\n",
    "                    names.append(name)\n",
    "                elif type == 'false_positive' and label == 0 and pred == 1:\n",
    "                    images.append(img)\n",
    "                    labels.append(label)\n",
    "                    preds.append(pred)\n",
    "                    names.append(name)\n",
    "                if len(images) >= num_images:\n",
    "                    return images, labels, preds, names\n",
    "    return images, labels, preds, names\n",
    "\n",
    "types = ['true_negative', 'true_positive', 'false_negative', 'false_positive']\n",
    "for type in types:\n",
    "    images, labels, preds, names = get_images_by_type(valloader, resnet, device, type)\n",
    "    display_images(images, labels, preds, names, type.replace('_', ' ').title())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
